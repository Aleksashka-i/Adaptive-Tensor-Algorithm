{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320909af",
   "metadata": {},
   "source": [
    "### 0. Validate pretrained\n",
    "Берём предобученную ResNet32 (``pretrained_models/resnet32-d509ac18.th``). Посмотрим на accuracy у исходной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e43711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test: [0/79]\tTime 9.200 (9.200)\tLoss 0.2115 (0.2115)\tPrec@1 93.750 (93.750)\n",
      "Test: [50/79]\tTime 0.332 (0.538)\tLoss 0.4470 (0.3828)\tPrec@1 90.625 (92.371)\n",
      " * Prec@1 92.630\n",
      " * Timec@1 0.493\n"
     ]
    }
   ],
   "source": [
    "%run stage_0/validate_pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c6e6a8",
   "metadata": {},
   "source": [
    "### 1. Get initial weights\n",
    "Берём предобученную ResNet32 (``pretrained_models/resnet32-d509ac18.th``). Вытаскиваем веса, кладём тензоры в ``weights/weigths_base.mat``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb13f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stage_1/get_initial_weights.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c9b08",
   "metadata": {},
   "source": [
    "Затем расскладываем веса в CP-decomposition, использую NLS (non-linear least squares), с помощью ``cpd_nls`` из [Tensorlab](https://www.tensorlab.net) в MATLAB (примерный скрипт в ``MATLAB/script_matlab_decompose.m``). В файле ``weights/weights_nls.mat``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e83d49",
   "metadata": {},
   "source": [
    "### 2. Fine-tune initial decomposition\n",
    "Заменяем набор слоёв в моделе на декомпозированные (все фильтры 3x3) (``weigths/weigths_nls.mat``) с помощью ``cpd_nls``. Затем делаем файнтюнинг весов (``epochs=50``). Лучшая модель сохранена в ``decomposed/best_initial_decompose.th``. Заменённые слои можно посмотреть в ``functional.py``, в примере всего 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stage_2/fine_tune_initial_decomposition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791906cb",
   "metadata": {},
   "source": [
    "![initial_decomposition](stage_2/initial_decomposition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42a97b",
   "metadata": {},
   "source": [
    "Можно запустить ``stage_2/check.py``, чтобы посмотреть на структуру модели и accuracy на тестовой выборке y ``decompose/best_initial_decompose.th`` (это быстро). Хуже на 1.1 от исходной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b162bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(8, 8, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=8, bias=False)\n",
      "          (2_decomposed): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=8, bias=False)\n",
      "          (3_decomposed): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(8, 8, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=8, bias=False)\n",
      "          (2_decomposed): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=8, bias=False)\n",
      "          (3_decomposed): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=16, bias=False)\n",
      "          (2_decomposed): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=16, bias=False)\n",
      "          (3_decomposed): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=16, bias=False)\n",
      "          (2_decomposed): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=16, bias=False)\n",
      "          (3_decomposed): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=32, bias=False)\n",
      "          (2_decomposed): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=32, bias=False)\n",
      "          (3_decomposed): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=32, bias=False)\n",
      "          (2_decomposed): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=32, bias=False)\n",
      "          (3_decomposed): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test: [0/79]\tTime 7.342 (7.342)\tLoss 0.3062 (0.3062)\tPrec@1 92.969 (92.969)\n",
      "Test: [50/79]\tTime 0.303 (0.508)\tLoss 0.5858 (0.4408)\tPrec@1 90.625 (91.330)\n",
      " * Prec@1 91.520\n",
      " * Timec@1 0.430\n"
     ]
    }
   ],
   "source": [
    "%run stage_2/check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad32a56",
   "metadata": {},
   "source": [
    "### 3. Extend Decomposed Kernels\n",
    "Берём отфайнтьюниную модель с предыдущего шага (``decomposed/best_initital_decompose.th``). Увеличиваем факторы (``1_decomposed``, ``2_decomposed``) до размеров 1x21 и 21x1. Добавляем сигмы ``resnet_with_sigmas.py`` и обучаем вместе с сигмами. Модель с лучшим приближением сохранена в ``decompose/best_extended_decompose.th``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57435a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stage_3/extend_decomposed_kernels.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8cbeab",
   "metadata": {},
   "source": [
    "![best_extended_decomposition](stage_3/extended_kernels_decomposition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ec90f",
   "metadata": {},
   "source": [
    "![sigmas](stage_3/sigmas_values.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc4a8d",
   "metadata": {},
   "source": [
    "Можно запустить ``stage_3/check.py``, чтобы посмотреть на структуру модели, значения сигм, соответствующие им размеры ядер и и accuracy на тестовой выборке y ``decompose/best_extended_decompose.th`` (это быстро). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ecd83bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(8, 8, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=8, bias=False)\n",
      "          (2_decomposed): Conv2d(8, 8, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=8, bias=False)\n",
      "          (3_decomposed): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(8, 8, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=8, bias=False)\n",
      "          (2_decomposed): Conv2d(8, 8, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=8, bias=False)\n",
      "          (3_decomposed): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(16, 16, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=16, bias=False)\n",
      "          (2_decomposed): Conv2d(16, 16, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=16, bias=False)\n",
      "          (3_decomposed): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(16, 16, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=16, bias=False)\n",
      "          (2_decomposed): Conv2d(16, 16, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=16, bias=False)\n",
      "          (3_decomposed): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(32, 32, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=32, bias=False)\n",
      "          (2_decomposed): Conv2d(32, 32, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=32, bias=False)\n",
      "          (3_decomposed): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Sequential(\n",
      "          (0_decomposed): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1_decomposed): Conv2d(32, 32, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=32, bias=False)\n",
      "          (2_decomposed): Conv2d(32, 32, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=32, bias=False)\n",
      "          (3_decomposed): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best sigmas:  [-1.0235847234725952, -1.6244574785232544, -0.6324220895767212, -1.0306727886199951, -0.5999158024787903, -0.45050451159477234]\n",
      "best kernels_sz:  [9, 7, 11, 9, 11, 11]\n",
      "Files already downloaded and verified\n",
      "Test: [0/79]\tTime 7.086 (7.086)\tLoss 0.2054 (0.2054)\tPrec@1 91.406 (91.406)\n",
      "Test: [50/79]\tTime 0.432 (0.575)\tLoss 0.4664 (0.3881)\tPrec@1 85.938 (88.817)\n",
      " * Prec@1 89.050\n",
      " * Timec@1 0.520\n"
     ]
    }
   ],
   "source": [
    "%run stage_3/check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fc104",
   "metadata": {},
   "source": [
    "### 4. Get decomposed weights\n",
    "Берём предобученную полученную на предыдущем шаге модель, применяем к весам маску и кропаем их до полученных размеров. Затем кладём декомпозиции весов в ``weights/decomposed_weights.mat``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a6d0caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernels_sz:  [9, 7, 11, 9, 11, 11]\n",
      "sigmas:  [-1.0235847234725952, -1.6244574785232544, -0.6324220895767212, -1.0306727886199951, -0.5999158024787903, -0.45050451159477234]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "\n",
    "%run stage_4/get_decomposed_weights.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51364fc7",
   "metadata": {},
   "source": [
    "Затем обратно композируем тензоры с помощью ``cpdgen`` из [Tensorlab](https://www.tensorlab.net) в MATLAB (примерный скрипт в ``MATLAB/script_matlab_compose.m``). В файле ``weights/weights_composed.mat``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d820c",
   "metadata": {},
   "source": [
    "### 5. Final fine-tune\n",
    "Заменяем набор слоёв в моделе на нормальные ``conv2`` с новыми размерами фильтров и весами, скомпозированными на предыдущем шаге. Делаем финальный файнтьюнинг (``epochs=50``). Лучшая модель сохранена в ``decomposed/best_final.th``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d71248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Files already downloaded and verified\n",
      "kernels_sz:  [9, 7, 11, 9, 11, 11]\n",
      "Test: [0/79]\tTime 7.305 (7.305)\tLoss 0.2054 (0.2054)\tPrec@1 91.406 (91.406)\n",
      "Test: [50/79]\tTime 0.466 (0.566)\tLoss 0.4664 (0.3881)\tPrec@1 85.938 (88.817)\n",
      " * Prec@1 89.050\n",
      " * Timec@1 0.516\n",
      "Epoch: [0][0/391]\tTime 12.319 (12.319)\tData 9.634 (9.634)\tLoss 0.1851 (0.1851)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [0][50/391]\tTime 2.023 (2.778)\tData 0.000 (0.189)\tLoss 0.2052 (0.1870)\tPrec@1 90.625 (93.842)\n",
      "Epoch: [0][100/391]\tTime 2.820 (2.719)\tData 0.001 (0.096)\tLoss 0.1535 (0.1905)\tPrec@1 95.312 (93.634)\n",
      "Epoch: [0][150/391]\tTime 2.891 (2.752)\tData 0.002 (0.064)\tLoss 0.1670 (0.1853)\tPrec@1 94.531 (93.740)\n",
      "Epoch: [0][200/391]\tTime 2.648 (2.780)\tData 0.000 (0.048)\tLoss 0.1739 (0.1848)\tPrec@1 93.750 (93.672)\n",
      "Epoch: [0][250/391]\tTime 2.705 (2.826)\tData 0.000 (0.039)\tLoss 0.1981 (0.1822)\tPrec@1 93.750 (93.722)\n",
      "Epoch: [0][300/391]\tTime 2.037 (2.811)\tData 0.000 (0.033)\tLoss 0.1938 (0.1809)\tPrec@1 93.750 (93.779)\n",
      "Epoch: [0][350/391]\tTime 3.155 (2.783)\tData 0.001 (0.028)\tLoss 0.1020 (0.1788)\tPrec@1 96.094 (93.832)\n",
      "Test: [0/79]\tTime 8.164 (8.164)\tLoss 0.1837 (0.1837)\tPrec@1 96.094 (96.094)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/79]\tTime 0.404 (0.577)\tLoss 0.3990 (0.3361)\tPrec@1 90.625 (90.119)\n",
      " * Prec@1 90.080\n",
      " * Timec@1 0.520\n",
      "best precision:  90.08\n",
      "Epoch: [1][0/391]\tTime 12.782 (12.782)\tData 10.005 (10.005)\tLoss 0.0851 (0.0851)\tPrec@1 98.438 (98.438)\n",
      "Epoch: [1][50/391]\tTime 2.878 (2.925)\tData 0.001 (0.197)\tLoss 0.2397 (0.1652)\tPrec@1 90.625 (94.393)\n",
      "Epoch: [1][100/391]\tTime 2.787 (2.848)\tData 0.001 (0.100)\tLoss 0.1388 (0.1692)\tPrec@1 96.094 (94.144)\n",
      "Epoch: [1][150/391]\tTime 3.821 (2.807)\tData 0.000 (0.067)\tLoss 0.1967 (0.1674)\tPrec@1 92.188 (94.221)\n",
      "Epoch: [1][200/391]\tTime 2.650 (2.842)\tData 0.002 (0.050)\tLoss 0.1502 (0.1692)\tPrec@1 93.750 (94.213)\n",
      "Epoch: [1][250/391]\tTime 2.799 (2.807)\tData 0.000 (0.041)\tLoss 0.1251 (0.1661)\tPrec@1 94.531 (94.273)\n",
      "Epoch: [1][300/391]\tTime 2.893 (2.851)\tData 0.001 (0.034)\tLoss 0.1617 (0.1642)\tPrec@1 92.969 (94.321)\n",
      "Epoch: [1][350/391]\tTime 2.852 (2.861)\tData 0.001 (0.029)\tLoss 0.1448 (0.1630)\tPrec@1 96.094 (94.351)\n",
      "Test: [0/79]\tTime 8.000 (8.000)\tLoss 0.1636 (0.1636)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.471 (0.652)\tLoss 0.3700 (0.3212)\tPrec@1 86.719 (90.135)\n",
      " * Prec@1 90.240\n",
      " * Timec@1 0.588\n",
      "best precision:  90.24\n",
      "Epoch: [2][0/391]\tTime 13.794 (13.794)\tData 10.663 (10.663)\tLoss 0.1054 (0.1054)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [2][50/391]\tTime 3.592 (3.301)\tData 0.001 (0.210)\tLoss 0.1535 (0.1516)\tPrec@1 92.969 (94.792)\n",
      "Epoch: [2][100/391]\tTime 2.912 (3.162)\tData 0.000 (0.107)\tLoss 0.1550 (0.1544)\tPrec@1 92.188 (94.732)\n",
      "Epoch: [2][150/391]\tTime 2.927 (3.098)\tData 0.002 (0.072)\tLoss 0.2013 (0.1571)\tPrec@1 92.188 (94.567)\n",
      "Epoch: [2][200/391]\tTime 2.854 (3.063)\tData 0.001 (0.054)\tLoss 0.1370 (0.1563)\tPrec@1 94.531 (94.558)\n",
      "Epoch: [2][250/391]\tTime 2.404 (2.990)\tData 0.000 (0.044)\tLoss 0.1547 (0.1565)\tPrec@1 94.531 (94.581)\n",
      "Epoch: [2][300/391]\tTime 2.757 (2.975)\tData 0.001 (0.036)\tLoss 0.2153 (0.1562)\tPrec@1 96.094 (94.627)\n",
      "Epoch: [2][350/391]\tTime 2.930 (2.959)\tData 0.001 (0.031)\tLoss 0.1959 (0.1563)\tPrec@1 93.750 (94.600)\n",
      "Test: [0/79]\tTime 7.939 (7.939)\tLoss 0.1762 (0.1762)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.455 (0.583)\tLoss 0.3595 (0.3141)\tPrec@1 89.844 (90.579)\n",
      " * Prec@1 90.510\n",
      " * Timec@1 0.524\n",
      "best precision:  90.51\n",
      "Epoch: [3][0/391]\tTime 13.904 (13.904)\tData 10.987 (10.987)\tLoss 0.1352 (0.1352)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [3][50/391]\tTime 2.028 (2.938)\tData 0.000 (0.216)\tLoss 0.1365 (0.1536)\tPrec@1 93.750 (94.822)\n",
      "Epoch: [3][100/391]\tTime 2.691 (2.798)\tData 0.001 (0.109)\tLoss 0.1060 (0.1518)\tPrec@1 96.094 (94.972)\n",
      "Epoch: [3][150/391]\tTime 2.772 (2.803)\tData 0.000 (0.073)\tLoss 0.1749 (0.1538)\tPrec@1 92.969 (94.899)\n",
      "Epoch: [3][200/391]\tTime 2.779 (2.834)\tData 0.001 (0.055)\tLoss 0.2045 (0.1546)\tPrec@1 92.188 (94.869)\n",
      "Epoch: [3][250/391]\tTime 3.814 (2.955)\tData 0.001 (0.045)\tLoss 0.1374 (0.1542)\tPrec@1 95.312 (94.883)\n",
      "Epoch: [3][300/391]\tTime 2.190 (3.047)\tData 0.000 (0.037)\tLoss 0.1911 (0.1528)\tPrec@1 92.188 (94.887)\n",
      "Epoch: [3][350/391]\tTime 2.582 (3.000)\tData 0.001 (0.032)\tLoss 0.1705 (0.1531)\tPrec@1 93.750 (94.863)\n",
      "Test: [0/79]\tTime 7.348 (7.348)\tLoss 0.1934 (0.1934)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.454 (0.552)\tLoss 0.3606 (0.3132)\tPrec@1 89.844 (90.426)\n",
      " * Prec@1 90.490\n",
      " * Timec@1 0.501\n",
      "best precision:  90.51\n",
      "Epoch: [4][0/391]\tTime 12.609 (12.609)\tData 9.920 (9.920)\tLoss 0.2185 (0.2185)\tPrec@1 90.625 (90.625)\n",
      "Epoch: [4][50/391]\tTime 2.656 (2.944)\tData 0.000 (0.195)\tLoss 0.0820 (0.1605)\tPrec@1 96.875 (94.409)\n",
      "Epoch: [4][100/391]\tTime 3.496 (2.884)\tData 0.001 (0.099)\tLoss 0.1796 (0.1558)\tPrec@1 94.531 (94.562)\n",
      "Epoch: [4][150/391]\tTime 2.959 (2.832)\tData 0.000 (0.066)\tLoss 0.1635 (0.1558)\tPrec@1 95.312 (94.645)\n",
      "Epoch: [4][200/391]\tTime 2.653 (2.781)\tData 0.001 (0.050)\tLoss 0.1117 (0.1505)\tPrec@1 95.312 (94.834)\n",
      "Epoch: [4][250/391]\tTime 2.699 (2.759)\tData 0.000 (0.040)\tLoss 0.1680 (0.1490)\tPrec@1 94.531 (94.889)\n",
      "Epoch: [4][300/391]\tTime 2.652 (2.745)\tData 0.001 (0.034)\tLoss 0.1262 (0.1486)\tPrec@1 96.875 (94.934)\n",
      "Epoch: [4][350/391]\tTime 2.644 (2.735)\tData 0.000 (0.029)\tLoss 0.1843 (0.1473)\tPrec@1 93.750 (94.972)\n",
      "Test: [0/79]\tTime 7.136 (7.136)\tLoss 0.1712 (0.1712)\tPrec@1 93.750 (93.750)\n",
      "Test: [50/79]\tTime 0.451 (0.588)\tLoss 0.3493 (0.3091)\tPrec@1 91.406 (90.518)\n",
      " * Prec@1 90.570\n",
      " * Timec@1 0.532\n",
      "best precision:  90.57\n",
      "Epoch: [5][0/391]\tTime 12.669 (12.669)\tData 9.869 (9.869)\tLoss 0.1986 (0.1986)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [5][50/391]\tTime 2.644 (2.866)\tData 0.000 (0.194)\tLoss 0.1489 (0.1402)\tPrec@1 93.750 (95.175)\n",
      "Epoch: [5][100/391]\tTime 2.658 (2.792)\tData 0.000 (0.098)\tLoss 0.0854 (0.1416)\tPrec@1 99.219 (95.328)\n",
      "Epoch: [5][150/391]\tTime 2.596 (2.745)\tData 0.000 (0.066)\tLoss 0.1294 (0.1455)\tPrec@1 95.312 (95.126)\n",
      "Epoch: [5][200/391]\tTime 2.667 (2.719)\tData 0.000 (0.050)\tLoss 0.1341 (0.1432)\tPrec@1 96.094 (95.219)\n",
      "Epoch: [5][250/391]\tTime 2.028 (2.675)\tData 0.000 (0.040)\tLoss 0.1752 (0.1441)\tPrec@1 93.750 (95.166)\n",
      "Epoch: [5][300/391]\tTime 1.983 (2.574)\tData 0.000 (0.033)\tLoss 0.1765 (0.1448)\tPrec@1 92.969 (95.076)\n",
      "Epoch: [5][350/391]\tTime 1.989 (2.490)\tData 0.000 (0.029)\tLoss 0.1058 (0.1441)\tPrec@1 95.312 (95.077)\n",
      "Test: [0/79]\tTime 6.126 (6.126)\tLoss 0.1708 (0.1708)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.330 (0.439)\tLoss 0.3215 (0.3076)\tPrec@1 89.844 (90.625)\n",
      " * Prec@1 90.590\n",
      " * Timec@1 0.397\n",
      "best precision:  90.59\n",
      "Epoch: [6][0/391]\tTime 9.675 (9.675)\tData 7.552 (7.552)\tLoss 0.1532 (0.1532)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [6][50/391]\tTime 2.035 (2.181)\tData 0.000 (0.148)\tLoss 0.1097 (0.1393)\tPrec@1 95.312 (95.144)\n",
      "Epoch: [6][100/391]\tTime 2.023 (2.105)\tData 0.000 (0.075)\tLoss 0.1109 (0.1387)\tPrec@1 96.875 (95.158)\n",
      "Epoch: [6][150/391]\tTime 2.052 (2.081)\tData 0.000 (0.050)\tLoss 0.1729 (0.1402)\tPrec@1 94.531 (95.183)\n",
      "Epoch: [6][200/391]\tTime 2.037 (2.068)\tData 0.000 (0.038)\tLoss 0.1638 (0.1390)\tPrec@1 95.312 (95.192)\n",
      "Epoch: [6][250/391]\tTime 2.021 (2.060)\tData 0.000 (0.030)\tLoss 0.1484 (0.1390)\tPrec@1 94.531 (95.225)\n",
      "Epoch: [6][300/391]\tTime 2.047 (2.055)\tData 0.000 (0.025)\tLoss 0.2170 (0.1407)\tPrec@1 90.625 (95.152)\n",
      "Epoch: [6][350/391]\tTime 2.036 (2.052)\tData 0.000 (0.022)\tLoss 0.1079 (0.1413)\tPrec@1 97.656 (95.128)\n",
      "Test: [0/79]\tTime 6.031 (6.031)\tLoss 0.1655 (0.1655)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.332 (0.442)\tLoss 0.3362 (0.3026)\tPrec@1 91.406 (90.702)\n",
      " * Prec@1 90.800\n",
      " * Timec@1 0.399\n",
      "best precision:  90.8\n",
      "Epoch: [7][0/391]\tTime 10.150 (10.150)\tData 8.060 (8.060)\tLoss 0.1295 (0.1295)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [7][50/391]\tTime 2.048 (2.188)\tData 0.000 (0.158)\tLoss 0.1118 (0.1307)\tPrec@1 96.875 (95.772)\n",
      "Epoch: [7][100/391]\tTime 2.034 (2.113)\tData 0.000 (0.080)\tLoss 0.1612 (0.1405)\tPrec@1 95.312 (95.398)\n",
      "Epoch: [7][150/391]\tTime 2.039 (2.087)\tData 0.000 (0.054)\tLoss 0.1173 (0.1377)\tPrec@1 96.094 (95.468)\n",
      "Epoch: [7][200/391]\tTime 2.031 (2.075)\tData 0.000 (0.040)\tLoss 0.1482 (0.1375)\tPrec@1 93.750 (95.410)\n",
      "Epoch: [7][250/391]\tTime 2.028 (2.067)\tData 0.000 (0.032)\tLoss 0.1054 (0.1378)\tPrec@1 96.875 (95.356)\n",
      "Epoch: [7][300/391]\tTime 2.050 (2.062)\tData 0.000 (0.027)\tLoss 0.1844 (0.1400)\tPrec@1 91.406 (95.279)\n",
      "Epoch: [7][350/391]\tTime 2.046 (2.059)\tData 0.000 (0.023)\tLoss 0.1501 (0.1405)\tPrec@1 93.750 (95.284)\n",
      "Test: [0/79]\tTime 6.077 (6.077)\tLoss 0.1568 (0.1568)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.332 (0.445)\tLoss 0.3507 (0.3046)\tPrec@1 88.281 (90.671)\n",
      " * Prec@1 90.710\n",
      " * Timec@1 0.401\n",
      "best precision:  90.8\n",
      "Epoch: [8][0/391]\tTime 9.681 (9.681)\tData 7.575 (7.575)\tLoss 0.1854 (0.1854)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [8][50/391]\tTime 2.028 (2.186)\tData 0.000 (0.149)\tLoss 0.1439 (0.1385)\tPrec@1 93.750 (95.021)\n",
      "Epoch: [8][100/391]\tTime 2.034 (2.113)\tData 0.000 (0.075)\tLoss 0.1182 (0.1379)\tPrec@1 95.312 (95.158)\n",
      "Epoch: [8][150/391]\tTime 2.035 (2.088)\tData 0.000 (0.050)\tLoss 0.1258 (0.1391)\tPrec@1 96.875 (95.230)\n",
      "Epoch: [8][200/391]\tTime 2.041 (2.075)\tData 0.000 (0.038)\tLoss 0.1727 (0.1417)\tPrec@1 93.750 (95.083)\n",
      "Epoch: [8][250/391]\tTime 2.050 (2.067)\tData 0.000 (0.030)\tLoss 0.1231 (0.1412)\tPrec@1 93.750 (95.120)\n",
      "Epoch: [8][300/391]\tTime 2.049 (2.062)\tData 0.000 (0.025)\tLoss 0.1591 (0.1393)\tPrec@1 93.750 (95.183)\n",
      "Epoch: [8][350/391]\tTime 2.018 (2.059)\tData 0.000 (0.022)\tLoss 0.1212 (0.1390)\tPrec@1 95.312 (95.217)\n",
      "Test: [0/79]\tTime 6.048 (6.048)\tLoss 0.1632 (0.1632)\tPrec@1 94.531 (94.531)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/79]\tTime 0.333 (0.444)\tLoss 0.3590 (0.3024)\tPrec@1 89.844 (90.564)\n",
      " * Prec@1 90.690\n",
      " * Timec@1 0.401\n",
      "best precision:  90.8\n",
      "Epoch: [9][0/391]\tTime 9.761 (9.761)\tData 7.667 (7.667)\tLoss 0.1564 (0.1564)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [9][50/391]\tTime 2.025 (2.189)\tData 0.000 (0.151)\tLoss 0.1008 (0.1420)\tPrec@1 96.875 (95.221)\n",
      "Epoch: [9][100/391]\tTime 2.031 (2.110)\tData 0.000 (0.076)\tLoss 0.1681 (0.1348)\tPrec@1 93.750 (95.429)\n",
      "Epoch: [9][150/391]\tTime 2.036 (2.084)\tData 0.000 (0.051)\tLoss 0.1482 (0.1333)\tPrec@1 95.312 (95.494)\n",
      "Epoch: [9][200/391]\tTime 2.026 (2.071)\tData 0.000 (0.038)\tLoss 0.1173 (0.1327)\tPrec@1 96.875 (95.503)\n",
      "Epoch: [9][250/391]\tTime 2.035 (2.063)\tData 0.000 (0.031)\tLoss 0.1211 (0.1343)\tPrec@1 95.312 (95.440)\n",
      "Epoch: [9][300/391]\tTime 2.021 (2.058)\tData 0.000 (0.026)\tLoss 0.1343 (0.1359)\tPrec@1 95.312 (95.416)\n",
      "Epoch: [9][350/391]\tTime 2.034 (2.054)\tData 0.000 (0.022)\tLoss 0.1286 (0.1359)\tPrec@1 95.312 (95.430)\n",
      "Test: [0/79]\tTime 6.071 (6.071)\tLoss 0.1746 (0.1746)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.322 (0.435)\tLoss 0.3436 (0.2991)\tPrec@1 90.625 (90.732)\n",
      " * Prec@1 90.750\n",
      " * Timec@1 0.392\n",
      "best precision:  90.8\n",
      "Epoch: [10][0/391]\tTime 9.639 (9.639)\tData 7.599 (7.599)\tLoss 0.1244 (0.1244)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [10][50/391]\tTime 2.025 (2.130)\tData 0.000 (0.149)\tLoss 0.1457 (0.1291)\tPrec@1 96.094 (95.650)\n",
      "Epoch: [10][100/391]\tTime 2.023 (2.079)\tData 0.000 (0.076)\tLoss 0.2073 (0.1357)\tPrec@1 91.406 (95.351)\n",
      "Epoch: [10][150/391]\tTime 2.035 (2.061)\tData 0.000 (0.051)\tLoss 0.2091 (0.1361)\tPrec@1 92.969 (95.369)\n",
      "Epoch: [10][200/391]\tTime 2.027 (2.053)\tData 0.000 (0.038)\tLoss 0.1490 (0.1347)\tPrec@1 95.312 (95.363)\n",
      "Epoch: [10][250/391]\tTime 2.029 (2.048)\tData 0.000 (0.031)\tLoss 0.1367 (0.1330)\tPrec@1 92.969 (95.418)\n",
      "Epoch: [10][300/391]\tTime 2.023 (2.044)\tData 0.000 (0.026)\tLoss 0.0867 (0.1324)\tPrec@1 97.656 (95.489)\n",
      "Epoch: [10][350/391]\tTime 2.025 (2.042)\tData 0.000 (0.022)\tLoss 0.1261 (0.1342)\tPrec@1 93.750 (95.444)\n",
      "Test: [0/79]\tTime 6.048 (6.048)\tLoss 0.1638 (0.1638)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.331 (0.440)\tLoss 0.3325 (0.2958)\tPrec@1 90.625 (90.778)\n",
      " * Prec@1 90.900\n",
      " * Timec@1 0.398\n",
      "best precision:  90.9\n",
      "Epoch: [11][0/391]\tTime 9.637 (9.637)\tData 7.527 (7.527)\tLoss 0.1033 (0.1033)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [11][50/391]\tTime 2.020 (2.176)\tData 0.000 (0.148)\tLoss 0.1512 (0.1329)\tPrec@1 95.312 (95.481)\n",
      "Epoch: [11][100/391]\tTime 2.026 (2.103)\tData 0.000 (0.075)\tLoss 0.0988 (0.1361)\tPrec@1 98.438 (95.305)\n",
      "Epoch: [11][150/391]\tTime 2.019 (2.078)\tData 0.000 (0.050)\tLoss 0.1003 (0.1345)\tPrec@1 96.094 (95.380)\n",
      "Epoch: [11][200/391]\tTime 2.034 (2.066)\tData 0.000 (0.038)\tLoss 0.1850 (0.1337)\tPrec@1 94.531 (95.464)\n",
      "Epoch: [11][250/391]\tTime 2.032 (2.059)\tData 0.000 (0.030)\tLoss 0.1417 (0.1350)\tPrec@1 96.094 (95.403)\n",
      "Epoch: [11][300/391]\tTime 2.034 (2.054)\tData 0.000 (0.025)\tLoss 0.0799 (0.1344)\tPrec@1 97.656 (95.429)\n",
      "Epoch: [11][350/391]\tTime 2.034 (2.050)\tData 0.000 (0.022)\tLoss 0.0716 (0.1343)\tPrec@1 99.219 (95.457)\n",
      "Test: [0/79]\tTime 6.090 (6.090)\tLoss 0.1624 (0.1624)\tPrec@1 93.750 (93.750)\n",
      "Test: [50/79]\tTime 0.331 (0.442)\tLoss 0.3395 (0.2952)\tPrec@1 90.625 (90.855)\n",
      " * Prec@1 90.920\n",
      " * Timec@1 0.399\n",
      "best precision:  90.92\n",
      "Epoch: [12][0/391]\tTime 9.646 (9.646)\tData 7.533 (7.533)\tLoss 0.1481 (0.1481)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [12][50/391]\tTime 2.026 (2.178)\tData 0.000 (0.148)\tLoss 0.1285 (0.1330)\tPrec@1 96.094 (95.542)\n",
      "Epoch: [12][100/391]\tTime 2.032 (2.103)\tData 0.000 (0.075)\tLoss 0.1108 (0.1345)\tPrec@1 96.094 (95.436)\n",
      "Epoch: [12][150/391]\tTime 2.035 (2.079)\tData 0.000 (0.050)\tLoss 0.1168 (0.1329)\tPrec@1 96.094 (95.571)\n",
      "Epoch: [12][200/391]\tTime 2.033 (2.066)\tData 0.000 (0.038)\tLoss 0.1341 (0.1336)\tPrec@1 96.875 (95.526)\n",
      "Epoch: [12][250/391]\tTime 2.035 (2.058)\tData 0.000 (0.030)\tLoss 0.1407 (0.1332)\tPrec@1 93.750 (95.512)\n",
      "Epoch: [12][300/391]\tTime 2.023 (2.053)\tData 0.000 (0.025)\tLoss 0.1592 (0.1344)\tPrec@1 93.750 (95.471)\n",
      "Epoch: [12][350/391]\tTime 2.029 (2.050)\tData 0.000 (0.022)\tLoss 0.1212 (0.1342)\tPrec@1 96.875 (95.466)\n",
      "Test: [0/79]\tTime 6.033 (6.033)\tLoss 0.1637 (0.1637)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.329 (0.442)\tLoss 0.3397 (0.2952)\tPrec@1 89.062 (90.686)\n",
      " * Prec@1 90.860\n",
      " * Timec@1 0.399\n",
      "best precision:  90.92\n",
      "Epoch: [13][0/391]\tTime 9.834 (9.834)\tData 7.742 (7.742)\tLoss 0.2135 (0.2135)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [13][50/391]\tTime 2.035 (2.180)\tData 0.000 (0.152)\tLoss 0.1347 (0.1335)\tPrec@1 96.094 (95.450)\n",
      "Epoch: [13][100/391]\tTime 2.029 (2.108)\tData 0.000 (0.077)\tLoss 0.1312 (0.1315)\tPrec@1 96.094 (95.575)\n",
      "Epoch: [13][150/391]\tTime 2.028 (2.079)\tData 0.000 (0.052)\tLoss 0.2543 (0.1329)\tPrec@1 90.625 (95.530)\n",
      "Epoch: [13][200/391]\tTime 2.032 (2.067)\tData 0.000 (0.039)\tLoss 0.0581 (0.1317)\tPrec@1 99.219 (95.651)\n",
      "Epoch: [13][250/391]\tTime 2.042 (2.059)\tData 0.000 (0.031)\tLoss 0.1641 (0.1297)\tPrec@1 94.531 (95.686)\n",
      "Epoch: [13][300/391]\tTime 2.036 (2.054)\tData 0.000 (0.026)\tLoss 0.1695 (0.1297)\tPrec@1 96.094 (95.650)\n",
      "Epoch: [13][350/391]\tTime 2.034 (2.051)\tData 0.000 (0.022)\tLoss 0.2124 (0.1301)\tPrec@1 92.188 (95.631)\n",
      "Test: [0/79]\tTime 6.193 (6.193)\tLoss 0.1641 (0.1641)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.334 (0.448)\tLoss 0.3442 (0.2989)\tPrec@1 89.062 (90.763)\n",
      " * Prec@1 90.860\n",
      " * Timec@1 0.403\n",
      "best precision:  90.92\n",
      "Epoch: [14][0/391]\tTime 9.657 (9.657)\tData 7.573 (7.573)\tLoss 0.1348 (0.1348)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [14][50/391]\tTime 2.035 (2.181)\tData 0.000 (0.149)\tLoss 0.1066 (0.1276)\tPrec@1 97.656 (95.711)\n",
      "Epoch: [14][100/391]\tTime 2.037 (2.106)\tData 0.000 (0.075)\tLoss 0.0975 (0.1267)\tPrec@1 95.312 (95.823)\n",
      "Epoch: [14][150/391]\tTime 2.031 (2.079)\tData 0.000 (0.050)\tLoss 0.1033 (0.1276)\tPrec@1 96.094 (95.814)\n",
      "Epoch: [14][200/391]\tTime 2.039 (2.066)\tData 0.000 (0.038)\tLoss 0.1210 (0.1283)\tPrec@1 96.094 (95.783)\n",
      "Epoch: [14][250/391]\tTime 2.025 (2.058)\tData 0.000 (0.030)\tLoss 0.1961 (0.1298)\tPrec@1 92.188 (95.708)\n",
      "Epoch: [14][300/391]\tTime 1.993 (2.052)\tData 0.000 (0.025)\tLoss 0.1725 (0.1290)\tPrec@1 92.969 (95.728)\n",
      "Epoch: [14][350/391]\tTime 1.972 (2.041)\tData 0.000 (0.022)\tLoss 0.1465 (0.1293)\tPrec@1 94.531 (95.678)\n",
      "Test: [0/79]\tTime 6.065 (6.065)\tLoss 0.1587 (0.1587)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.339 (0.446)\tLoss 0.3350 (0.2949)\tPrec@1 89.844 (90.916)\n",
      " * Prec@1 90.900\n",
      " * Timec@1 0.403\n",
      "best precision:  90.92\n",
      "Epoch: [15][0/391]\tTime 9.606 (9.606)\tData 7.537 (7.537)\tLoss 0.0459 (0.0459)\tPrec@1 100.000 (100.000)\n",
      "Epoch: [15][50/391]\tTime 2.025 (2.176)\tData 0.000 (0.148)\tLoss 0.1261 (0.1163)\tPrec@1 96.094 (96.232)\n",
      "Epoch: [15][100/391]\tTime 2.039 (2.108)\tData 0.000 (0.075)\tLoss 0.1649 (0.1211)\tPrec@1 93.750 (95.993)\n",
      "Epoch: [15][150/391]\tTime 2.022 (2.082)\tData 0.000 (0.050)\tLoss 0.1032 (0.1224)\tPrec@1 96.875 (95.928)\n",
      "Epoch: [15][200/391]\tTime 2.016 (2.069)\tData 0.000 (0.038)\tLoss 0.0991 (0.1254)\tPrec@1 97.656 (95.841)\n",
      "Epoch: [15][250/391]\tTime 2.024 (2.059)\tData 0.000 (0.030)\tLoss 0.1130 (0.1248)\tPrec@1 96.875 (95.873)\n",
      "Epoch: [15][300/391]\tTime 2.028 (2.054)\tData 0.000 (0.025)\tLoss 0.0630 (0.1250)\tPrec@1 96.875 (95.871)\n",
      "Epoch: [15][350/391]\tTime 2.026 (2.051)\tData 0.000 (0.022)\tLoss 0.1795 (0.1257)\tPrec@1 94.531 (95.822)\n",
      "Test: [0/79]\tTime 6.214 (6.214)\tLoss 0.1507 (0.1507)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.332 (0.446)\tLoss 0.3369 (0.2920)\tPrec@1 90.625 (91.008)\n",
      " * Prec@1 91.090\n",
      " * Timec@1 0.402\n",
      "best precision:  91.09\n",
      "Epoch: [16][0/391]\tTime 9.770 (9.770)\tData 7.654 (7.654)\tLoss 0.1769 (0.1769)\tPrec@1 92.969 (92.969)\n",
      "Epoch: [16][50/391]\tTime 2.033 (2.178)\tData 0.000 (0.150)\tLoss 0.1709 (0.1303)\tPrec@1 95.312 (95.282)\n",
      "Epoch: [16][100/391]\tTime 2.035 (2.104)\tData 0.000 (0.076)\tLoss 0.1125 (0.1306)\tPrec@1 97.656 (95.405)\n",
      "Epoch: [16][150/391]\tTime 2.028 (2.078)\tData 0.000 (0.051)\tLoss 0.1526 (0.1288)\tPrec@1 95.312 (95.514)\n",
      "Epoch: [16][200/391]\tTime 2.027 (2.066)\tData 0.000 (0.038)\tLoss 0.1381 (0.1287)\tPrec@1 95.312 (95.456)\n",
      "Epoch: [16][250/391]\tTime 2.022 (2.058)\tData 0.000 (0.031)\tLoss 0.1042 (0.1295)\tPrec@1 96.094 (95.499)\n",
      "Epoch: [16][300/391]\tTime 2.030 (2.053)\tData 0.000 (0.026)\tLoss 0.0673 (0.1274)\tPrec@1 98.438 (95.562)\n",
      "Epoch: [16][350/391]\tTime 2.038 (2.050)\tData 0.000 (0.022)\tLoss 0.1398 (0.1287)\tPrec@1 95.312 (95.486)\n",
      "Test: [0/79]\tTime 6.041 (6.041)\tLoss 0.1503 (0.1503)\tPrec@1 94.531 (94.531)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/79]\tTime 0.330 (0.443)\tLoss 0.3460 (0.2941)\tPrec@1 89.844 (90.809)\n",
      " * Prec@1 90.910\n",
      " * Timec@1 0.400\n",
      "best precision:  91.09\n",
      "Epoch: [17][0/391]\tTime 9.643 (9.643)\tData 7.518 (7.518)\tLoss 0.1466 (0.1466)\tPrec@1 94.531 (94.531)\n",
      "Epoch: [17][50/391]\tTime 2.040 (2.176)\tData 0.000 (0.148)\tLoss 0.0951 (0.1256)\tPrec@1 98.438 (95.818)\n",
      "Epoch: [17][100/391]\tTime 2.034 (2.102)\tData 0.000 (0.075)\tLoss 0.1345 (0.1253)\tPrec@1 94.531 (95.831)\n",
      "Epoch: [17][150/391]\tTime 2.028 (2.077)\tData 0.000 (0.050)\tLoss 0.1019 (0.1257)\tPrec@1 96.875 (95.923)\n",
      "Epoch: [17][200/391]\tTime 2.021 (2.064)\tData 0.000 (0.038)\tLoss 0.0998 (0.1229)\tPrec@1 95.312 (95.997)\n",
      "Epoch: [17][250/391]\tTime 2.034 (2.057)\tData 0.000 (0.030)\tLoss 0.1288 (0.1243)\tPrec@1 96.094 (95.966)\n",
      "Epoch: [17][300/391]\tTime 2.032 (2.052)\tData 0.000 (0.025)\tLoss 0.1587 (0.1255)\tPrec@1 93.750 (95.935)\n",
      "Epoch: [17][350/391]\tTime 2.033 (2.049)\tData 0.000 (0.022)\tLoss 0.1395 (0.1253)\tPrec@1 95.312 (95.909)\n",
      "Test: [0/79]\tTime 6.030 (6.030)\tLoss 0.1560 (0.1560)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.332 (0.440)\tLoss 0.3474 (0.2895)\tPrec@1 89.844 (91.192)\n",
      " * Prec@1 91.220\n",
      " * Timec@1 0.397\n",
      "best precision:  91.22\n",
      "Epoch: [18][0/391]\tTime 9.595 (9.595)\tData 7.510 (7.510)\tLoss 0.1275 (0.1275)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [18][50/391]\tTime 2.031 (2.178)\tData 0.000 (0.148)\tLoss 0.1438 (0.1266)\tPrec@1 96.094 (95.987)\n",
      "Epoch: [18][100/391]\tTime 2.026 (2.106)\tData 0.000 (0.075)\tLoss 0.1124 (0.1240)\tPrec@1 96.875 (95.970)\n",
      "Epoch: [18][150/391]\tTime 2.031 (2.081)\tData 0.000 (0.050)\tLoss 0.2059 (0.1227)\tPrec@1 93.750 (95.985)\n",
      "Epoch: [18][200/391]\tTime 2.031 (2.068)\tData 0.000 (0.038)\tLoss 0.1714 (0.1231)\tPrec@1 92.188 (95.950)\n",
      "Epoch: [18][250/391]\tTime 2.036 (2.061)\tData 0.000 (0.030)\tLoss 0.1185 (0.1221)\tPrec@1 95.312 (95.969)\n",
      "Epoch: [18][300/391]\tTime 2.038 (2.056)\tData 0.000 (0.025)\tLoss 0.1595 (0.1222)\tPrec@1 92.969 (95.941)\n",
      "Epoch: [18][350/391]\tTime 2.039 (2.052)\tData 0.000 (0.022)\tLoss 0.0768 (0.1235)\tPrec@1 98.438 (95.867)\n",
      "Test: [0/79]\tTime 6.229 (6.229)\tLoss 0.1537 (0.1537)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.332 (0.446)\tLoss 0.3418 (0.2896)\tPrec@1 90.625 (91.023)\n",
      " * Prec@1 91.140\n",
      " * Timec@1 0.402\n",
      "best precision:  91.22\n",
      "Epoch: [19][0/391]\tTime 9.713 (9.713)\tData 7.599 (7.599)\tLoss 0.1676 (0.1676)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [19][50/391]\tTime 2.024 (2.181)\tData 0.000 (0.149)\tLoss 0.1276 (0.1306)\tPrec@1 96.875 (95.665)\n",
      "Epoch: [19][100/391]\tTime 2.025 (2.106)\tData 0.000 (0.076)\tLoss 0.0862 (0.1297)\tPrec@1 97.656 (95.715)\n",
      "Epoch: [19][150/391]\tTime 2.034 (2.081)\tData 0.000 (0.051)\tLoss 0.1972 (0.1267)\tPrec@1 93.750 (95.721)\n",
      "Epoch: [19][200/391]\tTime 2.023 (2.069)\tData 0.000 (0.038)\tLoss 0.0651 (0.1263)\tPrec@1 99.219 (95.775)\n",
      "Epoch: [19][250/391]\tTime 2.038 (2.061)\tData 0.000 (0.031)\tLoss 0.1092 (0.1255)\tPrec@1 95.312 (95.839)\n",
      "Epoch: [19][300/391]\tTime 2.033 (2.056)\tData 0.000 (0.026)\tLoss 0.0935 (0.1246)\tPrec@1 96.875 (95.876)\n",
      "Epoch: [19][350/391]\tTime 2.031 (2.053)\tData 0.000 (0.022)\tLoss 0.0854 (0.1247)\tPrec@1 97.656 (95.851)\n",
      "Test: [0/79]\tTime 6.046 (6.046)\tLoss 0.1628 (0.1628)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.332 (0.443)\tLoss 0.3459 (0.2893)\tPrec@1 89.844 (91.008)\n",
      " * Prec@1 91.100\n",
      " * Timec@1 0.400\n",
      "best precision:  91.22\n",
      "Epoch: [20][0/391]\tTime 9.646 (9.646)\tData 7.553 (7.553)\tLoss 0.1200 (0.1200)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [20][50/391]\tTime 2.037 (2.179)\tData 0.000 (0.148)\tLoss 0.1187 (0.1250)\tPrec@1 95.312 (95.772)\n",
      "Epoch: [20][100/391]\tTime 2.037 (2.108)\tData 0.000 (0.075)\tLoss 0.0816 (0.1196)\tPrec@1 97.656 (95.985)\n",
      "Epoch: [20][150/391]\tTime 2.015 (2.080)\tData 0.000 (0.050)\tLoss 0.1063 (0.1170)\tPrec@1 96.875 (96.130)\n",
      "Epoch: [20][200/391]\tTime 2.044 (2.068)\tData 0.000 (0.038)\tLoss 0.1199 (0.1192)\tPrec@1 98.438 (96.043)\n",
      "Epoch: [20][250/391]\tTime 2.031 (2.060)\tData 0.000 (0.030)\tLoss 0.0808 (0.1195)\tPrec@1 97.656 (96.038)\n",
      "Epoch: [20][300/391]\tTime 2.020 (2.054)\tData 0.000 (0.025)\tLoss 0.1370 (0.1206)\tPrec@1 95.312 (95.990)\n",
      "Epoch: [20][350/391]\tTime 2.019 (2.050)\tData 0.000 (0.022)\tLoss 0.1302 (0.1203)\tPrec@1 96.094 (96.007)\n",
      "Test: [0/79]\tTime 6.035 (6.035)\tLoss 0.1587 (0.1587)\tPrec@1 93.750 (93.750)\n",
      "Test: [50/79]\tTime 0.333 (0.442)\tLoss 0.3412 (0.2893)\tPrec@1 89.062 (91.008)\n",
      " * Prec@1 91.050\n",
      " * Timec@1 0.399\n",
      "best precision:  91.22\n",
      "Epoch: [21][0/391]\tTime 9.651 (9.651)\tData 7.539 (7.539)\tLoss 0.1121 (0.1121)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [21][50/391]\tTime 2.019 (2.175)\tData 0.000 (0.148)\tLoss 0.1271 (0.1227)\tPrec@1 97.656 (95.864)\n",
      "Epoch: [21][100/391]\tTime 2.010 (2.102)\tData 0.000 (0.075)\tLoss 0.0734 (0.1224)\tPrec@1 97.656 (95.955)\n",
      "Epoch: [21][150/391]\tTime 2.015 (2.076)\tData 0.000 (0.050)\tLoss 0.1032 (0.1222)\tPrec@1 96.875 (95.892)\n",
      "Epoch: [21][200/391]\tTime 2.025 (2.064)\tData 0.000 (0.038)\tLoss 0.0590 (0.1217)\tPrec@1 98.438 (95.942)\n",
      "Epoch: [21][250/391]\tTime 1.994 (2.056)\tData 0.000 (0.030)\tLoss 0.0835 (0.1225)\tPrec@1 99.219 (95.923)\n",
      "Epoch: [21][300/391]\tTime 1.982 (2.046)\tData 0.000 (0.025)\tLoss 0.0751 (0.1227)\tPrec@1 97.656 (95.941)\n",
      "Epoch: [21][350/391]\tTime 1.978 (2.035)\tData 0.000 (0.022)\tLoss 0.1316 (0.1235)\tPrec@1 94.531 (95.891)\n",
      "Test: [0/79]\tTime 6.071 (6.071)\tLoss 0.1402 (0.1402)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.330 (0.438)\tLoss 0.3350 (0.2861)\tPrec@1 89.062 (91.131)\n",
      " * Prec@1 91.150\n",
      " * Timec@1 0.395\n",
      "best precision:  91.22\n",
      "Epoch: [22][0/391]\tTime 9.596 (9.596)\tData 7.551 (7.551)\tLoss 0.0753 (0.0753)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [22][50/391]\tTime 1.984 (2.140)\tData 0.000 (0.148)\tLoss 0.1185 (0.1276)\tPrec@1 94.531 (95.803)\n",
      "Epoch: [22][100/391]\tTime 1.994 (2.072)\tData 0.000 (0.075)\tLoss 0.1242 (0.1242)\tPrec@1 96.094 (95.947)\n",
      "Epoch: [22][150/391]\tTime 2.054 (2.051)\tData 0.000 (0.050)\tLoss 0.0876 (0.1213)\tPrec@1 98.438 (95.944)\n",
      "Epoch: [22][200/391]\tTime 2.039 (2.048)\tData 0.000 (0.038)\tLoss 0.1077 (0.1217)\tPrec@1 96.875 (95.911)\n",
      "Epoch: [22][250/391]\tTime 2.046 (2.046)\tData 0.000 (0.030)\tLoss 0.1206 (0.1195)\tPrec@1 96.094 (96.007)\n",
      "Epoch: [22][300/391]\tTime 2.035 (2.045)\tData 0.000 (0.025)\tLoss 0.1159 (0.1212)\tPrec@1 98.438 (95.943)\n",
      "Epoch: [22][350/391]\tTime 2.007 (2.043)\tData 0.000 (0.022)\tLoss 0.0979 (0.1206)\tPrec@1 96.875 (95.951)\n",
      "Test: [0/79]\tTime 6.216 (6.216)\tLoss 0.1450 (0.1450)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.333 (0.449)\tLoss 0.3442 (0.2874)\tPrec@1 89.844 (91.161)\n",
      " * Prec@1 91.120\n",
      " * Timec@1 0.405\n",
      "best precision:  91.22\n",
      "Epoch: [23][0/391]\tTime 9.794 (9.794)\tData 7.687 (7.687)\tLoss 0.1051 (0.1051)\tPrec@1 98.438 (98.438)\n",
      "Epoch: [23][50/391]\tTime 2.036 (2.187)\tData 0.000 (0.151)\tLoss 0.1830 (0.1229)\tPrec@1 93.750 (95.895)\n",
      "Epoch: [23][100/391]\tTime 2.044 (2.110)\tData 0.000 (0.076)\tLoss 0.0931 (0.1211)\tPrec@1 96.094 (96.009)\n",
      "Epoch: [23][150/391]\tTime 1.977 (2.084)\tData 0.000 (0.051)\tLoss 0.1962 (0.1198)\tPrec@1 94.531 (96.156)\n",
      "Epoch: [23][200/391]\tTime 1.984 (2.059)\tData 0.000 (0.039)\tLoss 0.1600 (0.1217)\tPrec@1 94.531 (96.067)\n",
      "Epoch: [23][250/391]\tTime 2.037 (2.046)\tData 0.000 (0.031)\tLoss 0.1015 (0.1220)\tPrec@1 96.875 (96.038)\n",
      "Epoch: [23][300/391]\tTime 2.037 (2.045)\tData 0.000 (0.026)\tLoss 0.1213 (0.1219)\tPrec@1 96.094 (96.021)\n",
      "Epoch: [23][350/391]\tTime 2.036 (2.043)\tData 0.000 (0.022)\tLoss 0.0855 (0.1232)\tPrec@1 96.875 (95.962)\n",
      "Test: [0/79]\tTime 6.033 (6.033)\tLoss 0.1465 (0.1465)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.327 (0.444)\tLoss 0.3387 (0.2839)\tPrec@1 89.844 (91.115)\n",
      " * Prec@1 91.200\n",
      " * Timec@1 0.401\n",
      "best precision:  91.22\n",
      "Epoch: [24][0/391]\tTime 9.796 (9.796)\tData 7.701 (7.701)\tLoss 0.1174 (0.1174)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [24][50/391]\tTime 2.036 (2.187)\tData 0.000 (0.151)\tLoss 0.1524 (0.1161)\tPrec@1 96.875 (96.048)\n",
      "Epoch: [24][100/391]\tTime 2.040 (2.112)\tData 0.000 (0.077)\tLoss 0.1082 (0.1165)\tPrec@1 95.312 (96.001)\n",
      "Epoch: [24][150/391]\tTime 2.036 (2.089)\tData 0.000 (0.051)\tLoss 0.0538 (0.1173)\tPrec@1 99.219 (96.078)\n",
      "Epoch: [24][200/391]\tTime 2.027 (2.076)\tData 0.000 (0.039)\tLoss 0.1085 (0.1177)\tPrec@1 97.656 (96.020)\n",
      "Epoch: [24][250/391]\tTime 2.038 (2.067)\tData 0.000 (0.031)\tLoss 0.1503 (0.1191)\tPrec@1 93.750 (95.944)\n",
      "Epoch: [24][300/391]\tTime 2.030 (2.061)\tData 0.000 (0.026)\tLoss 0.0898 (0.1202)\tPrec@1 97.656 (95.948)\n",
      "Epoch: [24][350/391]\tTime 2.025 (2.057)\tData 0.000 (0.022)\tLoss 0.1372 (0.1202)\tPrec@1 93.750 (95.931)\n",
      "Test: [0/79]\tTime 6.044 (6.044)\tLoss 0.1412 (0.1412)\tPrec@1 94.531 (94.531)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/79]\tTime 0.335 (0.441)\tLoss 0.3334 (0.2836)\tPrec@1 89.062 (91.222)\n",
      " * Prec@1 91.250\n",
      " * Timec@1 0.399\n",
      "best precision:  91.25\n",
      "Epoch: [25][0/391]\tTime 9.861 (9.861)\tData 7.757 (7.757)\tLoss 0.1164 (0.1164)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [25][50/391]\tTime 2.028 (2.180)\tData 0.000 (0.152)\tLoss 0.0667 (0.1271)\tPrec@1 100.000 (95.787)\n",
      "Epoch: [25][100/391]\tTime 2.023 (2.105)\tData 0.000 (0.077)\tLoss 0.1164 (0.1218)\tPrec@1 94.531 (95.908)\n",
      "Epoch: [25][150/391]\tTime 2.028 (2.080)\tData 0.000 (0.052)\tLoss 0.1430 (0.1197)\tPrec@1 93.750 (95.975)\n",
      "Epoch: [25][200/391]\tTime 2.024 (2.067)\tData 0.000 (0.039)\tLoss 0.0948 (0.1212)\tPrec@1 96.094 (95.903)\n",
      "Epoch: [25][250/391]\tTime 2.029 (2.060)\tData 0.000 (0.031)\tLoss 0.1196 (0.1204)\tPrec@1 95.312 (95.885)\n",
      "Epoch: [25][300/391]\tTime 2.030 (2.055)\tData 0.000 (0.026)\tLoss 0.1998 (0.1198)\tPrec@1 93.750 (95.909)\n",
      "Epoch: [25][350/391]\tTime 2.027 (2.052)\tData 0.000 (0.022)\tLoss 0.1443 (0.1195)\tPrec@1 94.531 (95.951)\n",
      "Test: [0/79]\tTime 6.035 (6.035)\tLoss 0.1467 (0.1467)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.330 (0.443)\tLoss 0.3290 (0.2828)\tPrec@1 90.625 (91.054)\n",
      " * Prec@1 91.150\n",
      " * Timec@1 0.400\n",
      "best precision:  91.25\n",
      "Epoch: [26][0/391]\tTime 9.607 (9.607)\tData 7.513 (7.513)\tLoss 0.1279 (0.1279)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [26][50/391]\tTime 2.032 (2.176)\tData 0.000 (0.148)\tLoss 0.0555 (0.1180)\tPrec@1 98.438 (96.109)\n",
      "Epoch: [26][100/391]\tTime 2.032 (2.103)\tData 0.000 (0.075)\tLoss 0.1258 (0.1171)\tPrec@1 95.312 (96.016)\n",
      "Epoch: [26][150/391]\tTime 2.028 (2.079)\tData 0.000 (0.050)\tLoss 0.1967 (0.1187)\tPrec@1 94.531 (96.021)\n",
      "Epoch: [26][200/391]\tTime 2.023 (2.069)\tData 0.000 (0.038)\tLoss 0.1174 (0.1185)\tPrec@1 96.094 (96.024)\n",
      "Epoch: [26][250/391]\tTime 2.026 (2.062)\tData 0.000 (0.030)\tLoss 0.1106 (0.1167)\tPrec@1 94.531 (96.159)\n",
      "Epoch: [26][300/391]\tTime 2.036 (2.057)\tData 0.000 (0.025)\tLoss 0.1271 (0.1170)\tPrec@1 94.531 (96.133)\n",
      "Epoch: [26][350/391]\tTime 2.023 (2.054)\tData 0.000 (0.022)\tLoss 0.1547 (0.1168)\tPrec@1 94.531 (96.149)\n",
      "Test: [0/79]\tTime 6.134 (6.134)\tLoss 0.1369 (0.1369)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.330 (0.444)\tLoss 0.3351 (0.2830)\tPrec@1 89.844 (91.085)\n",
      " * Prec@1 91.130\n",
      " * Timec@1 0.401\n",
      "best precision:  91.25\n",
      "Epoch: [27][0/391]\tTime 9.800 (9.800)\tData 7.703 (7.703)\tLoss 0.0765 (0.0765)\tPrec@1 99.219 (99.219)\n",
      "Epoch: [27][50/391]\tTime 2.031 (2.181)\tData 0.000 (0.151)\tLoss 0.1265 (0.1240)\tPrec@1 94.531 (96.155)\n",
      "Epoch: [27][100/391]\tTime 2.024 (2.108)\tData 0.000 (0.077)\tLoss 0.1172 (0.1206)\tPrec@1 96.875 (96.210)\n",
      "Epoch: [27][150/391]\tTime 2.023 (2.082)\tData 0.000 (0.051)\tLoss 0.0970 (0.1208)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [27][200/391]\tTime 2.031 (2.068)\tData 0.000 (0.039)\tLoss 0.1104 (0.1199)\tPrec@1 97.656 (96.102)\n",
      "Epoch: [27][250/391]\tTime 2.044 (2.061)\tData 0.000 (0.031)\tLoss 0.1152 (0.1188)\tPrec@1 95.312 (96.168)\n",
      "Epoch: [27][300/391]\tTime 2.042 (2.056)\tData 0.000 (0.026)\tLoss 0.1239 (0.1195)\tPrec@1 95.312 (96.099)\n",
      "Epoch: [27][350/391]\tTime 2.025 (2.052)\tData 0.000 (0.022)\tLoss 0.0862 (0.1195)\tPrec@1 97.656 (96.067)\n",
      "Test: [0/79]\tTime 6.046 (6.046)\tLoss 0.1358 (0.1358)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.332 (0.445)\tLoss 0.3409 (0.2837)\tPrec@1 89.062 (91.100)\n",
      " * Prec@1 91.090\n",
      " * Timec@1 0.402\n",
      "best precision:  91.25\n",
      "Epoch: [28][0/391]\tTime 9.715 (9.715)\tData 7.614 (7.614)\tLoss 0.0942 (0.0942)\tPrec@1 98.438 (98.438)\n",
      "Epoch: [28][50/391]\tTime 2.039 (2.182)\tData 0.000 (0.150)\tLoss 0.0622 (0.1013)\tPrec@1 98.438 (96.860)\n",
      "Epoch: [28][100/391]\tTime 2.035 (2.106)\tData 0.000 (0.076)\tLoss 0.1116 (0.1083)\tPrec@1 96.094 (96.674)\n",
      "Epoch: [28][150/391]\tTime 2.035 (2.081)\tData 0.000 (0.051)\tLoss 0.1349 (0.1130)\tPrec@1 93.750 (96.337)\n",
      "Epoch: [28][200/391]\tTime 2.014 (2.069)\tData 0.000 (0.038)\tLoss 0.0835 (0.1152)\tPrec@1 98.438 (96.199)\n",
      "Epoch: [28][250/391]\tTime 2.033 (2.063)\tData 0.000 (0.031)\tLoss 0.1274 (0.1160)\tPrec@1 95.312 (96.159)\n",
      "Epoch: [28][300/391]\tTime 2.013 (2.057)\tData 0.000 (0.026)\tLoss 0.0978 (0.1155)\tPrec@1 96.875 (96.192)\n",
      "Epoch: [28][350/391]\tTime 2.020 (2.053)\tData 0.000 (0.022)\tLoss 0.1212 (0.1161)\tPrec@1 93.750 (96.163)\n",
      "Test: [0/79]\tTime 6.137 (6.137)\tLoss 0.1431 (0.1431)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.330 (0.444)\tLoss 0.3243 (0.2803)\tPrec@1 89.062 (91.131)\n",
      " * Prec@1 91.170\n",
      " * Timec@1 0.401\n",
      "best precision:  91.25\n",
      "Epoch: [29][0/391]\tTime 9.690 (9.690)\tData 7.588 (7.588)\tLoss 0.1278 (0.1278)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [29][50/391]\tTime 2.033 (2.176)\tData 0.000 (0.149)\tLoss 0.1473 (0.1110)\tPrec@1 94.531 (96.247)\n",
      "Epoch: [29][100/391]\tTime 2.019 (2.102)\tData 0.000 (0.075)\tLoss 0.1091 (0.1193)\tPrec@1 96.875 (95.985)\n",
      "Epoch: [29][150/391]\tTime 2.029 (2.078)\tData 0.000 (0.051)\tLoss 0.0838 (0.1172)\tPrec@1 97.656 (96.089)\n",
      "Epoch: [29][200/391]\tTime 2.028 (2.065)\tData 0.000 (0.038)\tLoss 0.1329 (0.1152)\tPrec@1 96.094 (96.171)\n",
      "Epoch: [29][250/391]\tTime 2.022 (2.057)\tData 0.000 (0.031)\tLoss 0.1693 (0.1167)\tPrec@1 93.750 (96.109)\n",
      "Epoch: [29][300/391]\tTime 2.028 (2.052)\tData 0.000 (0.026)\tLoss 0.0777 (0.1179)\tPrec@1 98.438 (96.042)\n",
      "Epoch: [29][350/391]\tTime 2.030 (2.048)\tData 0.000 (0.022)\tLoss 0.1372 (0.1180)\tPrec@1 96.875 (96.076)\n",
      "Test: [0/79]\tTime 5.880 (5.880)\tLoss 0.1323 (0.1323)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.330 (0.440)\tLoss 0.3359 (0.2813)\tPrec@1 89.844 (91.222)\n",
      " * Prec@1 91.280\n",
      " * Timec@1 0.397\n",
      "best precision:  91.28\n",
      "Epoch: [30][0/391]\tTime 9.652 (9.652)\tData 7.551 (7.551)\tLoss 0.1444 (0.1444)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [30][50/391]\tTime 2.018 (2.175)\tData 0.000 (0.148)\tLoss 0.1527 (0.1216)\tPrec@1 94.531 (96.216)\n",
      "Epoch: [30][100/391]\tTime 2.024 (2.102)\tData 0.000 (0.075)\tLoss 0.0753 (0.1211)\tPrec@1 96.875 (96.179)\n",
      "Epoch: [30][150/391]\tTime 2.034 (2.077)\tData 0.000 (0.050)\tLoss 0.0763 (0.1180)\tPrec@1 98.438 (96.197)\n",
      "Epoch: [30][200/391]\tTime 2.032 (2.064)\tData 0.000 (0.038)\tLoss 0.1056 (0.1175)\tPrec@1 95.312 (96.117)\n",
      "Epoch: [30][250/391]\tTime 2.021 (2.058)\tData 0.001 (0.030)\tLoss 0.1213 (0.1179)\tPrec@1 96.875 (96.103)\n",
      "Epoch: [30][300/391]\tTime 2.024 (2.052)\tData 0.001 (0.025)\tLoss 0.0803 (0.1161)\tPrec@1 96.875 (96.182)\n",
      "Epoch: [30][350/391]\tTime 2.031 (2.048)\tData 0.000 (0.022)\tLoss 0.1385 (0.1169)\tPrec@1 96.875 (96.174)\n",
      "Test: [0/79]\tTime 6.223 (6.223)\tLoss 0.1477 (0.1477)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.330 (0.443)\tLoss 0.3403 (0.2829)\tPrec@1 89.062 (91.176)\n",
      " * Prec@1 91.250\n",
      " * Timec@1 0.399\n",
      "best precision:  91.28\n",
      "Epoch: [31][0/391]\tTime 9.787 (9.787)\tData 7.728 (7.728)\tLoss 0.1527 (0.1527)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [31][50/391]\tTime 2.024 (2.171)\tData 0.000 (0.152)\tLoss 0.0784 (0.1173)\tPrec@1 98.438 (96.063)\n",
      "Epoch: [31][100/391]\tTime 2.029 (2.098)\tData 0.000 (0.077)\tLoss 0.1832 (0.1176)\tPrec@1 95.312 (96.078)\n",
      "Epoch: [31][150/391]\tTime 2.019 (2.074)\tData 0.000 (0.051)\tLoss 0.0709 (0.1147)\tPrec@1 98.438 (96.135)\n",
      "Epoch: [31][200/391]\tTime 2.022 (2.062)\tData 0.000 (0.039)\tLoss 0.1363 (0.1159)\tPrec@1 96.094 (96.070)\n",
      "Epoch: [31][250/391]\tTime 2.030 (2.054)\tData 0.000 (0.031)\tLoss 0.0980 (0.1152)\tPrec@1 96.875 (96.165)\n",
      "Epoch: [31][300/391]\tTime 2.026 (2.050)\tData 0.000 (0.026)\tLoss 0.0845 (0.1150)\tPrec@1 96.875 (96.224)\n",
      "Epoch: [31][350/391]\tTime 1.998 (2.046)\tData 0.000 (0.022)\tLoss 0.1070 (0.1146)\tPrec@1 96.094 (96.232)\n",
      "Test: [0/79]\tTime 6.009 (6.009)\tLoss 0.1480 (0.1480)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.325 (0.441)\tLoss 0.3345 (0.2819)\tPrec@1 89.062 (91.422)\n",
      " * Prec@1 91.420\n",
      " * Timec@1 0.398\n",
      "best precision:  91.42\n",
      "Epoch: [32][0/391]\tTime 9.644 (9.644)\tData 7.567 (7.567)\tLoss 0.0999 (0.0999)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [32][50/391]\tTime 2.036 (2.174)\tData 0.000 (0.149)\tLoss 0.1158 (0.1159)\tPrec@1 97.656 (96.140)\n",
      "Epoch: [32][100/391]\tTime 2.029 (2.100)\tData 0.000 (0.075)\tLoss 0.1169 (0.1094)\tPrec@1 95.312 (96.426)\n",
      "Epoch: [32][150/391]\tTime 1.962 (2.069)\tData 0.000 (0.050)\tLoss 0.0924 (0.1103)\tPrec@1 97.656 (96.471)\n",
      "Epoch: [32][200/391]\tTime 1.982 (2.045)\tData 0.000 (0.038)\tLoss 0.1180 (0.1124)\tPrec@1 96.094 (96.451)\n",
      "Epoch: [32][250/391]\tTime 2.026 (2.032)\tData 0.000 (0.030)\tLoss 0.0902 (0.1146)\tPrec@1 96.094 (96.337)\n",
      "Epoch: [32][300/391]\tTime 2.040 (2.031)\tData 0.000 (0.025)\tLoss 0.1967 (0.1142)\tPrec@1 94.531 (96.291)\n",
      "Epoch: [32][350/391]\tTime 2.032 (2.030)\tData 0.000 (0.022)\tLoss 0.1283 (0.1153)\tPrec@1 96.094 (96.216)\n",
      "Test: [0/79]\tTime 6.039 (6.039)\tLoss 0.1466 (0.1466)\tPrec@1 94.531 (94.531)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/79]\tTime 0.332 (0.441)\tLoss 0.3383 (0.2819)\tPrec@1 89.844 (91.176)\n",
      " * Prec@1 91.190\n",
      " * Timec@1 0.398\n",
      "best precision:  91.42\n",
      "Epoch: [33][0/391]\tTime 9.622 (9.622)\tData 7.546 (7.546)\tLoss 0.1651 (0.1651)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [33][50/391]\tTime 2.010 (2.174)\tData 0.000 (0.148)\tLoss 0.0862 (0.1171)\tPrec@1 97.656 (95.987)\n",
      "Epoch: [33][100/391]\tTime 2.021 (2.101)\tData 0.000 (0.075)\tLoss 0.0937 (0.1132)\tPrec@1 97.656 (96.194)\n",
      "Epoch: [33][150/391]\tTime 2.030 (2.076)\tData 0.000 (0.050)\tLoss 0.1655 (0.1162)\tPrec@1 92.969 (96.145)\n",
      "Epoch: [33][200/391]\tTime 2.012 (2.064)\tData 0.000 (0.038)\tLoss 0.0865 (0.1172)\tPrec@1 96.094 (96.125)\n",
      "Epoch: [33][250/391]\tTime 2.023 (2.057)\tData 0.000 (0.030)\tLoss 0.0752 (0.1169)\tPrec@1 97.656 (96.122)\n",
      "Epoch: [33][300/391]\tTime 2.021 (2.051)\tData 0.000 (0.025)\tLoss 0.1625 (0.1171)\tPrec@1 94.531 (96.143)\n",
      "Epoch: [33][350/391]\tTime 2.029 (2.048)\tData 0.000 (0.022)\tLoss 0.0591 (0.1161)\tPrec@1 99.219 (96.169)\n",
      "Test: [0/79]\tTime 6.062 (6.062)\tLoss 0.1377 (0.1377)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.331 (0.442)\tLoss 0.3362 (0.2801)\tPrec@1 88.281 (91.284)\n",
      " * Prec@1 91.290\n",
      " * Timec@1 0.398\n",
      "best precision:  91.42\n",
      "Epoch: [34][0/391]\tTime 9.706 (9.706)\tData 7.604 (7.604)\tLoss 0.1249 (0.1249)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [34][50/391]\tTime 2.032 (2.173)\tData 0.000 (0.149)\tLoss 0.1138 (0.1078)\tPrec@1 95.312 (96.661)\n",
      "Epoch: [34][100/391]\tTime 2.028 (2.099)\tData 0.000 (0.076)\tLoss 0.1115 (0.1128)\tPrec@1 96.094 (96.334)\n",
      "Epoch: [34][150/391]\tTime 2.022 (2.074)\tData 0.000 (0.051)\tLoss 0.0946 (0.1153)\tPrec@1 96.094 (96.192)\n",
      "Epoch: [34][200/391]\tTime 2.019 (2.062)\tData 0.000 (0.038)\tLoss 0.1657 (0.1132)\tPrec@1 92.188 (96.222)\n",
      "Epoch: [34][250/391]\tTime 2.023 (2.055)\tData 0.000 (0.031)\tLoss 0.0768 (0.1124)\tPrec@1 96.875 (96.271)\n",
      "Epoch: [34][300/391]\tTime 2.020 (2.050)\tData 0.000 (0.026)\tLoss 0.0954 (0.1120)\tPrec@1 96.875 (96.278)\n",
      "Epoch: [34][350/391]\tTime 2.018 (2.046)\tData 0.000 (0.022)\tLoss 0.1192 (0.1125)\tPrec@1 97.656 (96.254)\n",
      "Test: [0/79]\tTime 6.051 (6.051)\tLoss 0.1323 (0.1323)\tPrec@1 96.094 (96.094)\n",
      "Test: [50/79]\tTime 0.329 (0.442)\tLoss 0.3281 (0.2793)\tPrec@1 89.844 (91.222)\n",
      " * Prec@1 91.360\n",
      " * Timec@1 0.399\n",
      "best precision:  91.42\n",
      "Epoch: [35][0/391]\tTime 9.615 (9.615)\tData 7.511 (7.511)\tLoss 0.0882 (0.0882)\tPrec@1 98.438 (98.438)\n",
      "Epoch: [35][50/391]\tTime 2.020 (2.170)\tData 0.000 (0.148)\tLoss 0.0987 (0.1164)\tPrec@1 96.875 (96.124)\n",
      "Epoch: [35][100/391]\tTime 2.070 (2.101)\tData 0.000 (0.075)\tLoss 0.0603 (0.1149)\tPrec@1 97.656 (96.225)\n",
      "Epoch: [35][150/391]\tTime 2.021 (2.079)\tData 0.000 (0.050)\tLoss 0.1144 (0.1131)\tPrec@1 96.875 (96.280)\n",
      "Epoch: [35][200/391]\tTime 2.031 (2.065)\tData 0.000 (0.038)\tLoss 0.1166 (0.1133)\tPrec@1 95.312 (96.276)\n",
      "Epoch: [35][250/391]\tTime 2.023 (2.057)\tData 0.000 (0.030)\tLoss 0.0916 (0.1114)\tPrec@1 97.656 (96.374)\n",
      "Epoch: [35][300/391]\tTime 2.019 (2.051)\tData 0.000 (0.025)\tLoss 0.0717 (0.1117)\tPrec@1 97.656 (96.333)\n",
      "Epoch: [35][350/391]\tTime 2.028 (2.047)\tData 0.000 (0.022)\tLoss 0.1033 (0.1120)\tPrec@1 98.438 (96.327)\n",
      "Test: [0/79]\tTime 6.226 (6.226)\tLoss 0.1312 (0.1312)\tPrec@1 96.094 (96.094)\n",
      "Test: [50/79]\tTime 0.331 (0.445)\tLoss 0.3392 (0.2787)\tPrec@1 89.844 (91.314)\n",
      " * Prec@1 91.390\n",
      " * Timec@1 0.400\n",
      "best precision:  91.42\n",
      "Epoch: [36][0/391]\tTime 9.845 (9.845)\tData 7.754 (7.754)\tLoss 0.0780 (0.0780)\tPrec@1 98.438 (98.438)\n",
      "Epoch: [36][50/391]\tTime 2.029 (2.176)\tData 0.000 (0.152)\tLoss 0.1317 (0.1133)\tPrec@1 93.750 (96.553)\n",
      "Epoch: [36][100/391]\tTime 2.024 (2.099)\tData 0.000 (0.077)\tLoss 0.0560 (0.1123)\tPrec@1 99.219 (96.597)\n",
      "Epoch: [36][150/391]\tTime 2.018 (2.074)\tData 0.000 (0.052)\tLoss 0.0675 (0.1124)\tPrec@1 97.656 (96.477)\n",
      "Epoch: [36][200/391]\tTime 2.015 (2.061)\tData 0.000 (0.039)\tLoss 0.1020 (0.1123)\tPrec@1 96.875 (96.420)\n",
      "Epoch: [36][250/391]\tTime 2.023 (2.053)\tData 0.000 (0.031)\tLoss 0.1488 (0.1126)\tPrec@1 92.969 (96.365)\n",
      "Epoch: [36][300/391]\tTime 2.015 (2.048)\tData 0.000 (0.026)\tLoss 0.1446 (0.1130)\tPrec@1 95.312 (96.338)\n",
      "Epoch: [36][350/391]\tTime 2.014 (2.045)\tData 0.000 (0.022)\tLoss 0.1416 (0.1135)\tPrec@1 96.094 (96.281)\n",
      "Test: [0/79]\tTime 6.066 (6.066)\tLoss 0.1297 (0.1297)\tPrec@1 96.094 (96.094)\n",
      "Test: [50/79]\tTime 0.329 (0.440)\tLoss 0.3277 (0.2782)\tPrec@1 89.062 (91.376)\n",
      " * Prec@1 91.470\n",
      " * Timec@1 0.397\n",
      "best precision:  91.47\n",
      "Epoch: [37][0/391]\tTime 9.782 (9.782)\tData 7.676 (7.676)\tLoss 0.0551 (0.0551)\tPrec@1 99.219 (99.219)\n",
      "Epoch: [37][50/391]\tTime 2.020 (2.174)\tData 0.000 (0.151)\tLoss 0.1227 (0.1194)\tPrec@1 96.875 (96.002)\n",
      "Epoch: [37][100/391]\tTime 2.026 (2.099)\tData 0.000 (0.076)\tLoss 0.1801 (0.1177)\tPrec@1 93.750 (96.047)\n",
      "Epoch: [37][150/391]\tTime 2.021 (2.075)\tData 0.000 (0.051)\tLoss 0.0503 (0.1160)\tPrec@1 98.438 (96.130)\n",
      "Epoch: [37][200/391]\tTime 2.034 (2.063)\tData 0.000 (0.039)\tLoss 0.2106 (0.1171)\tPrec@1 92.969 (96.117)\n",
      "Epoch: [37][250/391]\tTime 2.023 (2.056)\tData 0.000 (0.031)\tLoss 0.1371 (0.1153)\tPrec@1 92.969 (96.193)\n",
      "Epoch: [37][300/391]\tTime 2.027 (2.054)\tData 0.000 (0.026)\tLoss 0.1064 (0.1126)\tPrec@1 94.531 (96.270)\n",
      "Epoch: [37][350/391]\tTime 2.030 (2.050)\tData 0.000 (0.022)\tLoss 0.0892 (0.1121)\tPrec@1 97.656 (96.276)\n",
      "Test: [0/79]\tTime 6.135 (6.135)\tLoss 0.1236 (0.1236)\tPrec@1 97.656 (97.656)\n",
      "Test: [50/79]\tTime 0.328 (0.445)\tLoss 0.3342 (0.2789)\tPrec@1 89.844 (91.422)\n",
      " * Prec@1 91.490\n",
      " * Timec@1 0.401\n",
      "best precision:  91.49\n",
      "Epoch: [38][0/391]\tTime 9.673 (9.673)\tData 7.590 (7.590)\tLoss 0.0872 (0.0872)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [38][50/391]\tTime 1.968 (2.175)\tData 0.000 (0.149)\tLoss 0.1120 (0.1056)\tPrec@1 94.531 (96.569)\n",
      "Epoch: [38][100/391]\tTime 2.027 (2.101)\tData 0.000 (0.075)\tLoss 0.1067 (0.1073)\tPrec@1 97.656 (96.434)\n",
      "Epoch: [38][150/391]\tTime 2.029 (2.077)\tData 0.000 (0.051)\tLoss 0.1285 (0.1118)\tPrec@1 95.312 (96.233)\n",
      "Epoch: [38][200/391]\tTime 2.024 (2.065)\tData 0.000 (0.038)\tLoss 0.0980 (0.1112)\tPrec@1 96.094 (96.304)\n",
      "Epoch: [38][250/391]\tTime 2.015 (2.057)\tData 0.000 (0.031)\tLoss 0.1496 (0.1108)\tPrec@1 94.531 (96.312)\n",
      "Epoch: [38][300/391]\tTime 2.029 (2.052)\tData 0.000 (0.026)\tLoss 0.1259 (0.1105)\tPrec@1 93.750 (96.346)\n",
      "Epoch: [38][350/391]\tTime 2.021 (2.049)\tData 0.000 (0.022)\tLoss 0.1149 (0.1099)\tPrec@1 96.094 (96.363)\n",
      "Test: [0/79]\tTime 5.832 (5.832)\tLoss 0.1214 (0.1214)\tPrec@1 96.094 (96.094)\n",
      "Test: [50/79]\tTime 0.331 (0.440)\tLoss 0.3411 (0.2772)\tPrec@1 89.844 (91.422)\n",
      " * Prec@1 91.450\n",
      " * Timec@1 0.398\n",
      "best precision:  91.49\n",
      "Epoch: [39][0/391]\tTime 9.660 (9.660)\tData 7.555 (7.555)\tLoss 0.1095 (0.1095)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [39][50/391]\tTime 2.040 (2.176)\tData 0.000 (0.148)\tLoss 0.0991 (0.1156)\tPrec@1 96.875 (95.925)\n",
      "Epoch: [39][100/391]\tTime 2.030 (2.103)\tData 0.000 (0.075)\tLoss 0.1351 (0.1148)\tPrec@1 95.312 (96.032)\n",
      "Epoch: [39][150/391]\tTime 2.064 (2.078)\tData 0.000 (0.050)\tLoss 0.1096 (0.1136)\tPrec@1 95.312 (96.151)\n",
      "Epoch: [39][200/391]\tTime 1.975 (2.056)\tData 0.000 (0.038)\tLoss 0.0832 (0.1124)\tPrec@1 97.656 (96.249)\n",
      "Epoch: [39][250/391]\tTime 2.035 (2.041)\tData 0.000 (0.030)\tLoss 0.1075 (0.1122)\tPrec@1 96.875 (96.252)\n",
      "Epoch: [39][300/391]\tTime 2.017 (2.036)\tData 0.000 (0.025)\tLoss 0.1255 (0.1120)\tPrec@1 95.312 (96.275)\n",
      "Epoch: [39][350/391]\tTime 2.019 (2.034)\tData 0.000 (0.022)\tLoss 0.0717 (0.1119)\tPrec@1 98.438 (96.270)\n",
      "Test: [0/79]\tTime 6.252 (6.252)\tLoss 0.1436 (0.1436)\tPrec@1 92.969 (92.969)\n",
      "Test: [50/79]\tTime 0.326 (0.446)\tLoss 0.3352 (0.2785)\tPrec@1 89.844 (91.207)\n",
      " * Prec@1 91.270\n",
      " * Timec@1 0.401\n",
      "best precision:  91.49\n",
      "Epoch: [40][0/391]\tTime 9.763 (9.763)\tData 7.661 (7.661)\tLoss 0.0598 (0.0598)\tPrec@1 98.438 (98.438)\n",
      "Epoch: [40][50/391]\tTime 2.082 (2.190)\tData 0.000 (0.151)\tLoss 0.0865 (0.1138)\tPrec@1 96.875 (96.308)\n",
      "Epoch: [40][100/391]\tTime 2.039 (2.111)\tData 0.000 (0.076)\tLoss 0.0877 (0.1110)\tPrec@1 98.438 (96.457)\n",
      "Epoch: [40][150/391]\tTime 2.023 (2.084)\tData 0.000 (0.051)\tLoss 0.0918 (0.1106)\tPrec@1 96.094 (96.482)\n",
      "Epoch: [40][200/391]\tTime 2.019 (2.070)\tData 0.000 (0.038)\tLoss 0.1178 (0.1112)\tPrec@1 95.312 (96.412)\n",
      "Epoch: [40][250/391]\tTime 2.024 (2.066)\tData 0.000 (0.031)\tLoss 0.1900 (0.1114)\tPrec@1 94.531 (96.352)\n",
      "Epoch: [40][300/391]\tTime 2.026 (2.059)\tData 0.000 (0.026)\tLoss 0.1073 (0.1101)\tPrec@1 95.312 (96.384)\n",
      "Epoch: [40][350/391]\tTime 2.193 (2.057)\tData 0.001 (0.022)\tLoss 0.0884 (0.1112)\tPrec@1 97.656 (96.339)\n",
      "Test: [0/79]\tTime 7.296 (7.296)\tLoss 0.1332 (0.1332)\tPrec@1 95.312 (95.312)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/79]\tTime 0.479 (0.585)\tLoss 0.3490 (0.2786)\tPrec@1 89.062 (91.131)\n",
      " * Prec@1 91.160\n",
      " * Timec@1 0.535\n",
      "best precision:  91.49\n",
      "Epoch: [41][0/391]\tTime 11.868 (11.868)\tData 9.155 (9.155)\tLoss 0.0838 (0.0838)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [41][50/391]\tTime 2.708 (2.826)\tData 0.000 (0.180)\tLoss 0.1721 (0.1128)\tPrec@1 95.312 (96.431)\n",
      "Epoch: [41][100/391]\tTime 2.606 (2.731)\tData 0.000 (0.091)\tLoss 0.0992 (0.1133)\tPrec@1 96.094 (96.357)\n",
      "Epoch: [41][150/391]\tTime 2.776 (2.705)\tData 0.001 (0.061)\tLoss 0.1313 (0.1112)\tPrec@1 94.531 (96.352)\n",
      "Epoch: [41][200/391]\tTime 2.597 (2.694)\tData 0.000 (0.046)\tLoss 0.0989 (0.1106)\tPrec@1 96.875 (96.432)\n",
      "Epoch: [41][250/391]\tTime 1.969 (2.668)\tData 0.001 (0.037)\tLoss 0.1208 (0.1098)\tPrec@1 96.875 (96.464)\n",
      "Epoch: [41][300/391]\tTime 2.897 (2.655)\tData 0.001 (0.031)\tLoss 0.1745 (0.1101)\tPrec@1 93.750 (96.442)\n",
      "Epoch: [41][350/391]\tTime 2.641 (2.657)\tData 0.001 (0.027)\tLoss 0.1039 (0.1106)\tPrec@1 96.875 (96.423)\n",
      "Test: [0/79]\tTime 7.561 (7.561)\tLoss 0.1380 (0.1380)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.431 (0.560)\tLoss 0.3414 (0.2802)\tPrec@1 89.062 (91.391)\n",
      " * Prec@1 91.400\n",
      " * Timec@1 0.506\n",
      "best precision:  91.49\n",
      "Epoch: [42][0/391]\tTime 12.512 (12.512)\tData 9.820 (9.820)\tLoss 0.0683 (0.0683)\tPrec@1 97.656 (97.656)\n",
      "Epoch: [42][50/391]\tTime 2.670 (2.848)\tData 0.000 (0.193)\tLoss 0.0830 (0.1123)\tPrec@1 97.656 (96.247)\n",
      "Epoch: [42][100/391]\tTime 2.624 (2.656)\tData 0.001 (0.098)\tLoss 0.1064 (0.1140)\tPrec@1 97.656 (96.202)\n",
      "Epoch: [42][150/391]\tTime 2.755 (2.667)\tData 0.000 (0.066)\tLoss 0.1943 (0.1116)\tPrec@1 93.750 (96.270)\n",
      "Epoch: [42][200/391]\tTime 2.690 (2.668)\tData 0.001 (0.049)\tLoss 0.0928 (0.1118)\tPrec@1 97.656 (96.280)\n",
      "Epoch: [42][250/391]\tTime 2.662 (2.672)\tData 0.001 (0.040)\tLoss 0.0958 (0.1106)\tPrec@1 96.875 (96.346)\n",
      "Epoch: [42][300/391]\tTime 2.677 (2.673)\tData 0.001 (0.033)\tLoss 0.0817 (0.1101)\tPrec@1 97.656 (96.338)\n",
      "Epoch: [42][350/391]\tTime 2.224 (2.645)\tData 0.000 (0.029)\tLoss 0.1910 (0.1101)\tPrec@1 95.312 (96.370)\n",
      "Test: [0/79]\tTime 7.783 (7.783)\tLoss 0.1282 (0.1282)\tPrec@1 96.875 (96.875)\n",
      "Test: [50/79]\tTime 0.422 (0.566)\tLoss 0.3254 (0.2777)\tPrec@1 89.844 (91.406)\n",
      " * Prec@1 91.500\n",
      " * Timec@1 0.511\n",
      "best precision:  91.5\n",
      "Epoch: [43][0/391]\tTime 12.499 (12.499)\tData 9.758 (9.758)\tLoss 0.0616 (0.0616)\tPrec@1 98.438 (98.438)\n",
      "Epoch: [43][50/391]\tTime 2.720 (2.853)\tData 0.002 (0.192)\tLoss 0.1136 (0.1064)\tPrec@1 96.875 (96.553)\n",
      "Epoch: [43][100/391]\tTime 2.618 (2.756)\tData 0.000 (0.097)\tLoss 0.1272 (0.1093)\tPrec@1 97.656 (96.488)\n",
      "Epoch: [43][150/391]\tTime 2.027 (2.724)\tData 0.000 (0.065)\tLoss 0.1189 (0.1098)\tPrec@1 96.875 (96.373)\n",
      "Epoch: [43][200/391]\tTime 2.602 (2.672)\tData 0.001 (0.049)\tLoss 0.0821 (0.1117)\tPrec@1 96.875 (96.335)\n",
      "Epoch: [43][250/391]\tTime 2.604 (2.679)\tData 0.000 (0.039)\tLoss 0.1549 (0.1106)\tPrec@1 93.750 (96.380)\n",
      "Epoch: [43][300/391]\tTime 2.678 (2.678)\tData 0.000 (0.033)\tLoss 0.0574 (0.1094)\tPrec@1 97.656 (96.439)\n",
      "Epoch: [43][350/391]\tTime 2.789 (2.678)\tData 0.000 (0.028)\tLoss 0.1131 (0.1086)\tPrec@1 96.094 (96.459)\n",
      "Test: [0/79]\tTime 7.603 (7.603)\tLoss 0.1270 (0.1270)\tPrec@1 96.875 (96.875)\n",
      "Test: [50/79]\tTime 0.326 (0.534)\tLoss 0.3415 (0.2765)\tPrec@1 88.281 (91.406)\n",
      " * Prec@1 91.460\n",
      " * Timec@1 0.457\n",
      "best precision:  91.5\n",
      "Epoch: [44][0/391]\tTime 12.974 (12.974)\tData 9.985 (9.985)\tLoss 0.0949 (0.0949)\tPrec@1 96.875 (96.875)\n",
      "Epoch: [44][50/391]\tTime 2.767 (2.845)\tData 0.000 (0.197)\tLoss 0.0684 (0.1075)\tPrec@1 97.656 (96.308)\n",
      "Epoch: [44][100/391]\tTime 2.638 (2.951)\tData 0.000 (0.100)\tLoss 0.0835 (0.1095)\tPrec@1 97.656 (96.349)\n",
      "Epoch: [44][150/391]\tTime 3.181 (2.914)\tData 0.001 (0.067)\tLoss 0.0928 (0.1125)\tPrec@1 98.438 (96.208)\n",
      "Epoch: [44][200/391]\tTime 3.434 (2.901)\tData 0.002 (0.051)\tLoss 0.1032 (0.1097)\tPrec@1 95.312 (96.416)\n",
      "Epoch: [44][250/391]\tTime 3.221 (2.903)\tData 0.000 (0.041)\tLoss 0.1322 (0.1090)\tPrec@1 96.094 (96.383)\n",
      "Epoch: [44][300/391]\tTime 3.096 (2.918)\tData 0.001 (0.034)\tLoss 0.1252 (0.1078)\tPrec@1 96.094 (96.465)\n",
      "Epoch: [44][350/391]\tTime 2.694 (2.907)\tData 0.000 (0.030)\tLoss 0.0959 (0.1091)\tPrec@1 98.438 (96.430)\n",
      "Test: [0/79]\tTime 7.835 (7.835)\tLoss 0.1357 (0.1357)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.408 (0.569)\tLoss 0.3496 (0.2771)\tPrec@1 88.281 (91.284)\n",
      " * Prec@1 91.350\n",
      " * Timec@1 0.513\n",
      "best precision:  91.5\n",
      "Epoch: [45][0/391]\tTime 14.747 (14.747)\tData 10.775 (10.775)\tLoss 0.1600 (0.1600)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [45][50/391]\tTime 2.855 (3.014)\tData 0.001 (0.212)\tLoss 0.0957 (0.1132)\tPrec@1 96.875 (96.186)\n",
      "Epoch: [45][100/391]\tTime 2.096 (2.863)\tData 0.000 (0.108)\tLoss 0.0417 (0.1089)\tPrec@1 100.000 (96.357)\n",
      "Epoch: [45][150/391]\tTime 2.653 (2.837)\tData 0.000 (0.072)\tLoss 0.0494 (0.1068)\tPrec@1 99.219 (96.487)\n",
      "Epoch: [45][200/391]\tTime 4.034 (2.850)\tData 0.001 (0.054)\tLoss 0.0869 (0.1074)\tPrec@1 96.094 (96.409)\n",
      "Epoch: [45][250/391]\tTime 2.712 (2.849)\tData 0.000 (0.044)\tLoss 0.1217 (0.1073)\tPrec@1 96.094 (96.449)\n",
      "Epoch: [45][300/391]\tTime 2.659 (2.854)\tData 0.001 (0.037)\tLoss 0.1744 (0.1077)\tPrec@1 93.750 (96.460)\n",
      "Epoch: [45][350/391]\tTime 3.254 (2.832)\tData 0.000 (0.032)\tLoss 0.0818 (0.1073)\tPrec@1 96.875 (96.479)\n",
      "Test: [0/79]\tTime 8.172 (8.172)\tLoss 0.1306 (0.1306)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.696 (0.609)\tLoss 0.3413 (0.2791)\tPrec@1 90.625 (91.376)\n",
      " * Prec@1 91.420\n",
      " * Timec@1 0.553\n",
      "best precision:  91.5\n",
      "Epoch: [46][0/391]\tTime 14.373 (14.373)\tData 11.264 (11.264)\tLoss 0.1381 (0.1381)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [46][50/391]\tTime 2.802 (3.072)\tData 0.001 (0.222)\tLoss 0.1268 (0.1032)\tPrec@1 95.312 (96.676)\n",
      "Epoch: [46][100/391]\tTime 2.931 (3.006)\tData 0.001 (0.112)\tLoss 0.0503 (0.1070)\tPrec@1 99.219 (96.581)\n",
      "Epoch: [46][150/391]\tTime 4.300 (3.015)\tData 0.001 (0.075)\tLoss 0.1170 (0.1109)\tPrec@1 94.531 (96.409)\n",
      "Epoch: [46][200/391]\tTime 2.966 (2.956)\tData 0.002 (0.057)\tLoss 0.0905 (0.1080)\tPrec@1 96.094 (96.455)\n",
      "Epoch: [46][250/391]\tTime 3.150 (2.969)\tData 0.004 (0.046)\tLoss 0.0932 (0.1073)\tPrec@1 97.656 (96.452)\n",
      "Epoch: [46][300/391]\tTime 2.874 (2.979)\tData 0.002 (0.038)\tLoss 0.1043 (0.1075)\tPrec@1 96.875 (96.457)\n",
      "Epoch: [46][350/391]\tTime 2.951 (2.969)\tData 0.000 (0.033)\tLoss 0.0931 (0.1088)\tPrec@1 98.438 (96.396)\n",
      "Test: [0/79]\tTime 7.897 (7.897)\tLoss 0.1249 (0.1249)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.474 (0.592)\tLoss 0.3385 (0.2743)\tPrec@1 90.625 (91.437)\n",
      " * Prec@1 91.520\n",
      " * Timec@1 0.534\n",
      "best precision:  91.52\n",
      "Epoch: [47][0/391]\tTime 13.864 (13.864)\tData 10.353 (10.353)\tLoss 0.1522 (0.1522)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [47][50/391]\tTime 2.743 (2.898)\tData 0.001 (0.204)\tLoss 0.1031 (0.1050)\tPrec@1 97.656 (96.538)\n",
      "Epoch: [47][100/391]\tTime 2.806 (2.874)\tData 0.001 (0.103)\tLoss 0.0810 (0.1070)\tPrec@1 96.875 (96.519)\n",
      "Epoch: [47][150/391]\tTime 2.774 (2.863)\tData 0.000 (0.069)\tLoss 0.0896 (0.1105)\tPrec@1 97.656 (96.332)\n",
      "Epoch: [47][200/391]\tTime 2.672 (2.827)\tData 0.001 (0.052)\tLoss 0.0886 (0.1094)\tPrec@1 97.656 (96.343)\n",
      "Epoch: [47][250/391]\tTime 2.018 (2.795)\tData 0.000 (0.042)\tLoss 0.1207 (0.1084)\tPrec@1 96.094 (96.414)\n",
      "Epoch: [47][300/391]\tTime 2.747 (2.765)\tData 0.000 (0.035)\tLoss 0.1454 (0.1083)\tPrec@1 96.094 (96.462)\n",
      "Epoch: [47][350/391]\tTime 2.929 (2.774)\tData 0.000 (0.030)\tLoss 0.1034 (0.1083)\tPrec@1 95.312 (96.430)\n",
      "Test: [0/79]\tTime 7.959 (7.959)\tLoss 0.1243 (0.1243)\tPrec@1 95.312 (95.312)\n",
      "Test: [50/79]\tTime 0.506 (0.599)\tLoss 0.3342 (0.2751)\tPrec@1 91.406 (91.391)\n",
      " * Prec@1 91.480\n",
      " * Timec@1 0.538\n",
      "best precision:  91.52\n",
      "Epoch: [48][0/391]\tTime 16.148 (16.148)\tData 11.784 (11.784)\tLoss 0.1068 (0.1068)\tPrec@1 96.094 (96.094)\n",
      "Epoch: [48][50/391]\tTime 2.802 (3.361)\tData 0.001 (0.232)\tLoss 0.1028 (0.1009)\tPrec@1 96.875 (96.722)\n",
      "Epoch: [48][100/391]\tTime 2.019 (3.116)\tData 0.000 (0.118)\tLoss 0.1131 (0.1037)\tPrec@1 96.094 (96.744)\n",
      "Epoch: [48][150/391]\tTime 2.710 (2.976)\tData 0.000 (0.079)\tLoss 0.0821 (0.1042)\tPrec@1 96.875 (96.689)\n",
      "Epoch: [48][200/391]\tTime 2.919 (2.921)\tData 0.000 (0.060)\tLoss 0.0933 (0.1063)\tPrec@1 96.875 (96.583)\n",
      "Epoch: [48][250/391]\tTime 2.719 (2.918)\tData 0.000 (0.048)\tLoss 0.1273 (0.1072)\tPrec@1 96.875 (96.514)\n",
      "Epoch: [48][300/391]\tTime 2.792 (2.902)\tData 0.001 (0.040)\tLoss 0.0974 (0.1069)\tPrec@1 96.875 (96.501)\n",
      "Epoch: [48][350/391]\tTime 3.054 (2.877)\tData 0.001 (0.034)\tLoss 0.0673 (0.1068)\tPrec@1 97.656 (96.517)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/79]\tTime 8.375 (8.375)\tLoss 0.1370 (0.1370)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.462 (0.613)\tLoss 0.3376 (0.2745)\tPrec@1 90.625 (91.452)\n",
      " * Prec@1 91.550\n",
      " * Timec@1 0.555\n",
      "best precision:  91.55\n",
      "Epoch: [49][0/391]\tTime 16.061 (16.061)\tData 11.727 (11.727)\tLoss 0.1170 (0.1170)\tPrec@1 95.312 (95.312)\n",
      "Epoch: [49][50/391]\tTime 2.757 (3.229)\tData 0.000 (0.231)\tLoss 0.1123 (0.1123)\tPrec@1 96.094 (96.400)\n",
      "Epoch: [49][100/391]\tTime 2.835 (3.030)\tData 0.000 (0.117)\tLoss 0.0833 (0.1076)\tPrec@1 96.875 (96.473)\n",
      "Epoch: [49][150/391]\tTime 2.852 (2.966)\tData 0.000 (0.078)\tLoss 0.0699 (0.1085)\tPrec@1 96.875 (96.394)\n",
      "Epoch: [49][200/391]\tTime 3.926 (2.887)\tData 0.002 (0.059)\tLoss 0.1385 (0.1072)\tPrec@1 96.875 (96.506)\n",
      "Epoch: [49][250/391]\tTime 2.991 (2.897)\tData 0.002 (0.047)\tLoss 0.0841 (0.1063)\tPrec@1 96.875 (96.526)\n",
      "Epoch: [49][300/391]\tTime 2.754 (2.903)\tData 0.000 (0.040)\tLoss 0.0684 (0.1060)\tPrec@1 98.438 (96.538)\n",
      "Epoch: [49][350/391]\tTime 2.887 (2.900)\tData 0.002 (0.034)\tLoss 0.1094 (0.1065)\tPrec@1 96.094 (96.510)\n",
      "Test: [0/79]\tTime 11.477 (11.477)\tLoss 0.1372 (0.1372)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.424 (0.673)\tLoss 0.3408 (0.2740)\tPrec@1 89.844 (91.468)\n",
      " * Prec@1 91.490\n",
      " * Timec@1 0.591\n",
      "best precision:  91.55\n"
     ]
    }
   ],
   "source": [
    "%run stage_5/final_fine_tune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1b47b",
   "metadata": {},
   "source": [
    "![final](stage_5/final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24600d5",
   "metadata": {},
   "source": [
    "Можно запустить ``stage_5/check.py``, чтобы посмотреть на структуру модели и accuracy на тестовой выборке y ``decompose/best_final.th`` (это быстро). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c599db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+\n",
      "|           Modules            | Parameters |\n",
      "+------------------------------+------------+\n",
      "|     module.conv1.weight      |    432     |\n",
      "|      module.bn1.weight       |     16     |\n",
      "|       module.bn1.bias        |     16     |\n",
      "| module.layer1.0.conv1.weight |    2304    |\n",
      "|  module.layer1.0.bn1.weight  |     16     |\n",
      "|   module.layer1.0.bn1.bias   |     16     |\n",
      "| module.layer1.0.conv2.weight |    2304    |\n",
      "|  module.layer1.0.bn2.weight  |     16     |\n",
      "|   module.layer1.0.bn2.bias   |     16     |\n",
      "| module.layer1.1.conv1.weight |    2304    |\n",
      "|  module.layer1.1.bn1.weight  |     16     |\n",
      "|   module.layer1.1.bn1.bias   |     16     |\n",
      "| module.layer1.1.conv2.weight |    2304    |\n",
      "|  module.layer1.1.bn2.weight  |     16     |\n",
      "|   module.layer1.1.bn2.bias   |     16     |\n",
      "| module.layer1.2.conv1.weight |    2304    |\n",
      "|  module.layer1.2.bn1.weight  |     16     |\n",
      "|   module.layer1.2.bn1.bias   |     16     |\n",
      "| module.layer1.2.conv2.weight |    2304    |\n",
      "|  module.layer1.2.bn2.weight  |     16     |\n",
      "|   module.layer1.2.bn2.bias   |     16     |\n",
      "| module.layer1.3.conv1.weight |    2304    |\n",
      "|  module.layer1.3.bn1.weight  |     16     |\n",
      "|   module.layer1.3.bn1.bias   |     16     |\n",
      "| module.layer1.3.conv2.weight |    2304    |\n",
      "|  module.layer1.3.bn2.weight  |     16     |\n",
      "|   module.layer1.3.bn2.bias   |     16     |\n",
      "| module.layer1.4.conv1.weight |    2304    |\n",
      "|  module.layer1.4.bn1.weight  |     16     |\n",
      "|   module.layer1.4.bn1.bias   |     16     |\n",
      "| module.layer1.4.conv2.weight |    2304    |\n",
      "|  module.layer1.4.bn2.weight  |     16     |\n",
      "|   module.layer1.4.bn2.bias   |     16     |\n",
      "| module.layer2.0.conv1.weight |    4608    |\n",
      "|  module.layer2.0.bn1.weight  |     32     |\n",
      "|   module.layer2.0.bn1.bias   |     32     |\n",
      "| module.layer2.0.conv2.weight |    9216    |\n",
      "|  module.layer2.0.bn2.weight  |     32     |\n",
      "|   module.layer2.0.bn2.bias   |     32     |\n",
      "| module.layer2.1.conv1.weight |    9216    |\n",
      "|  module.layer2.1.bn1.weight  |     32     |\n",
      "|   module.layer2.1.bn1.bias   |     32     |\n",
      "| module.layer2.1.conv2.weight |    9216    |\n",
      "|  module.layer2.1.bn2.weight  |     32     |\n",
      "|   module.layer2.1.bn2.bias   |     32     |\n",
      "| module.layer2.2.conv1.weight |    9216    |\n",
      "|  module.layer2.2.bn1.weight  |     32     |\n",
      "|   module.layer2.2.bn1.bias   |     32     |\n",
      "| module.layer2.2.conv2.weight |    9216    |\n",
      "|  module.layer2.2.bn2.weight  |     32     |\n",
      "|   module.layer2.2.bn2.bias   |     32     |\n",
      "| module.layer2.3.conv1.weight |    9216    |\n",
      "|  module.layer2.3.bn1.weight  |     32     |\n",
      "|   module.layer2.3.bn1.bias   |     32     |\n",
      "| module.layer2.3.conv2.weight |    9216    |\n",
      "|  module.layer2.3.bn2.weight  |     32     |\n",
      "|   module.layer2.3.bn2.bias   |     32     |\n",
      "| module.layer2.4.conv1.weight |    9216    |\n",
      "|  module.layer2.4.bn1.weight  |     32     |\n",
      "|   module.layer2.4.bn1.bias   |     32     |\n",
      "| module.layer2.4.conv2.weight |    9216    |\n",
      "|  module.layer2.4.bn2.weight  |     32     |\n",
      "|   module.layer2.4.bn2.bias   |     32     |\n",
      "| module.layer3.0.conv1.weight |   18432    |\n",
      "|  module.layer3.0.bn1.weight  |     64     |\n",
      "|   module.layer3.0.bn1.bias   |     64     |\n",
      "| module.layer3.0.conv2.weight |   36864    |\n",
      "|  module.layer3.0.bn2.weight  |     64     |\n",
      "|   module.layer3.0.bn2.bias   |     64     |\n",
      "| module.layer3.1.conv1.weight |   36864    |\n",
      "|  module.layer3.1.bn1.weight  |     64     |\n",
      "|   module.layer3.1.bn1.bias   |     64     |\n",
      "| module.layer3.1.conv2.weight |   36864    |\n",
      "|  module.layer3.1.bn2.weight  |     64     |\n",
      "|   module.layer3.1.bn2.bias   |     64     |\n",
      "| module.layer3.2.conv1.weight |   36864    |\n",
      "|  module.layer3.2.bn1.weight  |     64     |\n",
      "|   module.layer3.2.bn1.bias   |     64     |\n",
      "| module.layer3.2.conv2.weight |   36864    |\n",
      "|  module.layer3.2.bn2.weight  |     64     |\n",
      "|   module.layer3.2.bn2.bias   |     64     |\n",
      "| module.layer3.3.conv1.weight |   36864    |\n",
      "|  module.layer3.3.bn1.weight  |     64     |\n",
      "|   module.layer3.3.bn1.bias   |     64     |\n",
      "| module.layer3.3.conv2.weight |   36864    |\n",
      "|  module.layer3.3.bn2.weight  |     64     |\n",
      "|   module.layer3.3.bn2.bias   |     64     |\n",
      "| module.layer3.4.conv1.weight |   36864    |\n",
      "|  module.layer3.4.bn1.weight  |     64     |\n",
      "|   module.layer3.4.bn1.bias   |     64     |\n",
      "| module.layer3.4.conv2.weight |   36864    |\n",
      "|  module.layer3.4.bn2.weight  |     64     |\n",
      "|   module.layer3.4.bn2.bias   |     64     |\n",
      "|     module.linear.weight     |    640     |\n",
      "|      module.linear.bias      |     10     |\n",
      "+------------------------------+------------+\n",
      "Total Trainable Params: 464154\n",
      "464154\n",
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+\n",
      "|           Modules            | Parameters |\n",
      "+------------------------------+------------+\n",
      "|     module.conv1.weight      |    432     |\n",
      "|      module.bn1.weight       |     16     |\n",
      "|       module.bn1.bias        |     16     |\n",
      "| module.layer1.0.conv1.weight |    2304    |\n",
      "|  module.layer1.0.bn1.weight  |     16     |\n",
      "|   module.layer1.0.bn1.bias   |     16     |\n",
      "| module.layer1.0.conv2.weight |    2304    |\n",
      "|  module.layer1.0.bn2.weight  |     16     |\n",
      "|   module.layer1.0.bn2.bias   |     16     |\n",
      "| module.layer1.1.conv1.weight |    2304    |\n",
      "|  module.layer1.1.bn1.weight  |     16     |\n",
      "|   module.layer1.1.bn1.bias   |     16     |\n",
      "| module.layer1.1.conv2.weight |   20736    |\n",
      "|  module.layer1.1.bn2.weight  |     16     |\n",
      "|   module.layer1.1.bn2.bias   |     16     |\n",
      "| module.layer1.2.conv1.weight |    2304    |\n",
      "|  module.layer1.2.bn1.weight  |     16     |\n",
      "|   module.layer1.2.bn1.bias   |     16     |\n",
      "| module.layer1.2.conv2.weight |    2304    |\n",
      "|  module.layer1.2.bn2.weight  |     16     |\n",
      "|   module.layer1.2.bn2.bias   |     16     |\n",
      "| module.layer1.3.conv1.weight |    2304    |\n",
      "|  module.layer1.3.bn1.weight  |     16     |\n",
      "|   module.layer1.3.bn1.bias   |     16     |\n",
      "| module.layer1.3.conv2.weight |   12544    |\n",
      "|  module.layer1.3.bn2.weight  |     16     |\n",
      "|   module.layer1.3.bn2.bias   |     16     |\n",
      "| module.layer1.4.conv1.weight |    2304    |\n",
      "|  module.layer1.4.bn1.weight  |     16     |\n",
      "|   module.layer1.4.bn1.bias   |     16     |\n",
      "| module.layer1.4.conv2.weight |    2304    |\n",
      "|  module.layer1.4.bn2.weight  |     16     |\n",
      "|   module.layer1.4.bn2.bias   |     16     |\n",
      "| module.layer2.0.conv1.weight |    4608    |\n",
      "|  module.layer2.0.bn1.weight  |     32     |\n",
      "|   module.layer2.0.bn1.bias   |     32     |\n",
      "| module.layer2.0.conv2.weight |    9216    |\n",
      "|  module.layer2.0.bn2.weight  |     32     |\n",
      "|   module.layer2.0.bn2.bias   |     32     |\n",
      "| module.layer2.1.conv1.weight |    9216    |\n",
      "|  module.layer2.1.bn1.weight  |     32     |\n",
      "|   module.layer2.1.bn1.bias   |     32     |\n",
      "| module.layer2.1.conv2.weight |   123904   |\n",
      "|  module.layer2.1.bn2.weight  |     32     |\n",
      "|   module.layer2.1.bn2.bias   |     32     |\n",
      "| module.layer2.2.conv1.weight |    9216    |\n",
      "|  module.layer2.2.bn1.weight  |     32     |\n",
      "|   module.layer2.2.bn1.bias   |     32     |\n",
      "| module.layer2.2.conv2.weight |    9216    |\n",
      "|  module.layer2.2.bn2.weight  |     32     |\n",
      "|   module.layer2.2.bn2.bias   |     32     |\n",
      "| module.layer2.3.conv1.weight |    9216    |\n",
      "|  module.layer2.3.bn1.weight  |     32     |\n",
      "|   module.layer2.3.bn1.bias   |     32     |\n",
      "| module.layer2.3.conv2.weight |   82944    |\n",
      "|  module.layer2.3.bn2.weight  |     32     |\n",
      "|   module.layer2.3.bn2.bias   |     32     |\n",
      "| module.layer2.4.conv1.weight |    9216    |\n",
      "|  module.layer2.4.bn1.weight  |     32     |\n",
      "|   module.layer2.4.bn1.bias   |     32     |\n",
      "| module.layer2.4.conv2.weight |    9216    |\n",
      "|  module.layer2.4.bn2.weight  |     32     |\n",
      "|   module.layer2.4.bn2.bias   |     32     |\n",
      "| module.layer3.0.conv1.weight |   18432    |\n",
      "|  module.layer3.0.bn1.weight  |     64     |\n",
      "|   module.layer3.0.bn1.bias   |     64     |\n",
      "| module.layer3.0.conv2.weight |   36864    |\n",
      "|  module.layer3.0.bn2.weight  |     64     |\n",
      "|   module.layer3.0.bn2.bias   |     64     |\n",
      "| module.layer3.1.conv1.weight |   36864    |\n",
      "|  module.layer3.1.bn1.weight  |     64     |\n",
      "|   module.layer3.1.bn1.bias   |     64     |\n",
      "| module.layer3.1.conv2.weight |   495616   |\n",
      "|  module.layer3.1.bn2.weight  |     64     |\n",
      "|   module.layer3.1.bn2.bias   |     64     |\n",
      "| module.layer3.2.conv1.weight |   36864    |\n",
      "|  module.layer3.2.bn1.weight  |     64     |\n",
      "|   module.layer3.2.bn1.bias   |     64     |\n",
      "| module.layer3.2.conv2.weight |   36864    |\n",
      "|  module.layer3.2.bn2.weight  |     64     |\n",
      "|   module.layer3.2.bn2.bias   |     64     |\n",
      "| module.layer3.3.conv1.weight |   36864    |\n",
      "|  module.layer3.3.bn1.weight  |     64     |\n",
      "|   module.layer3.3.bn1.bias   |     64     |\n",
      "| module.layer3.3.conv2.weight |   495616   |\n",
      "|  module.layer3.3.bn2.weight  |     64     |\n",
      "|   module.layer3.3.bn2.bias   |     64     |\n",
      "| module.layer3.4.conv1.weight |   36864    |\n",
      "|  module.layer3.4.bn1.weight  |     64     |\n",
      "|   module.layer3.4.bn1.bias   |     64     |\n",
      "| module.layer3.4.conv2.weight |   36864    |\n",
      "|  module.layer3.4.bn2.weight  |     64     |\n",
      "|   module.layer3.4.bn2.bias   |     64     |\n",
      "|     module.linear.weight     |    640     |\n",
      "|      module.linear.bias      |     10     |\n",
      "+------------------------------+------------+\n",
      "Total Trainable Params: 1598746\n",
      "1598746\n",
      "Files already downloaded and verified\n",
      "Test: [0/79]\tTime 5.046 (5.046)\tLoss 0.1370 (0.1370)\tPrec@1 94.531 (94.531)\n",
      "Test: [50/79]\tTime 0.328 (0.419)\tLoss 0.3376 (0.2745)\tPrec@1 90.625 (91.452)\n",
      " * Prec@1 91.550\n",
      " * Timec@1 0.383\n"
     ]
    }
   ],
   "source": [
    "%run stage_5/check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f65f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
